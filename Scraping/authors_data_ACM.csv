author,papers
Florent Benaych-Georges,[]
Frederique Crete,[]
Maryam Karimzadehgan,"[['Separate and Attend in Personal Email Search', 'In personal email search, user queries often impose different requirements on different aspects of the retrieved emails. For example, the query ""my recent flight to the US"" requires emails to be ranked based on both textual contents and recency of the email documents, while other queries such as ""medical history"" do not impose any constraints on the recency of the email. Recent deep learning-to-rank models for personal email search often directly concatenate dense numerical features (e.g., document age) with embedded sparse features (e.g., n-gram embeddings). In this paper, we first show with a set of experiments on synthetic datasets that direct concatenation of dense and sparse features does not lead to the optimal search performance of deep neural ranking models. To effectively incorporate both sparse and dense email features into personal email search ranking, we propose a novel neural model, SepAttn. SepAttn first builds two separate neural models to learn from sparse and dense features respectively, and then applies an attention mechanism at the prediction level to derive the final prediction from these two models. We conduct a comprehensive set of experiments on a large-scale email search dataset, and demonstrate that our SepAttn model consistently improves the search quality over the baseline models.'], ['Domain Adaptation for Enterprise Email Search', 'In the enterprise email search setting, the same search engine often powers multiple enterprises from various industries: technology, education, manufacturing, etc. However, using the same global ranking model across different enterprises may result in suboptimal search quality, due to the corpora differences and distinct information needs. On the other hand, training an individual ranking model for each enterprise may be infeasible, especially for smaller institutions with limited data. To address this data challenge, in this paper we propose a domain adaptation approach that fine-tunes the global model to each individual enterprise. In particular, we propose a novel application of the Maximum Mean Discrepancy (MMD) approach to information retrieval, which attempts to bridge the gap between the global data distribution and the data distribution for a given individual enterprise. We conduct a comprehensive set of experiments on a large-scale email search engine, and demonstrate that the MMD approach consistently improves the search quality for multiple individual domains, both in comparison to the global ranking model, as well as several competitive domain adaptation baselines including adversarial learning methods.'], ['Multi-Task Learning for Email Search Ranking with Auxiliary Query Clustering', 'User information needs vary significantly across different tasks, and therefore their queries will also differ considerably in their expressiveness and semantics. Many studies have been proposed to model such query diversity by obtaining query types and building query-dependent ranking models. These studies typically require either a labeled query dataset or clicks from multiple users aggregated over the same document. These techniques, however, are not applicable when manual query labeling is not viable, and aggregated clicks are unavailable due to the private nature of the document collection, e.g., in email search scenarios. In this paper, we study how to obtain query type in an unsupervised fashion and how to incorporate this information into query-dependent ranking models. We first develop a hierarchical clustering algorithm based on truncated SVD and varimax rotation to obtain coarse-to-fine query types. Then, we study three query-dependent ranking models, including two neural models that leverage query type information as additional features, and one novel multi-task neural model that views query type as the label for the auxiliary query cluster prediction task. This multi-task model is trained to simultaneously rank documents and predict query types. Our experiments on tens of millions of real-world email search queries demonstrate that the proposed multi-task model can significantly outperform the baseline neural ranking models, which either do not incorporate query type information or just simply feed query type as an additional feature.'], ['Statistical Translation Language Model for Twitter Search', 'With the prevalence of social media applications, an increasing number of internet users are actively publishing text information on-line. This influx provides a wealth of text information on those users. Ranking in social media poses different challenges than Web search ranking, one of which is that Microblog messages are really short. As a result, the vocabulary mismatch problem is exacerbated in social media search. In this paper, we first study the standard translation model for this problem and reveal that translation language model not only helps to bridge the vocabulary gap but also improves the estimate of Term Frequency. We further propose two ways to improve translation language model through leveraging Hashtag information and adaptively setting the self-translation parameter. Experimental results on Twitter data set show that our proposed methods are effective.'], ['A learning approach to optimizing exploration---exploitation tradeoff in relevance feedback', ""Relevance feedback is an effective technique for improving search accuracy in interactive information retrieval. In this paper, we study an interesting optimization problem in interactive feedback that aims at optimizing the tradeoff between presenting search results with the highest immediate utility to a user (but not necessarily most useful for collecting feedback information) and presenting search results with the best potential for collecting useful feedback information (but not necessarily the most useful documents from a user's perspective). Optimizing such an exploration---exploitation tradeoff is key to the optimization of the overall utility of relevance feedback to a user in the entire session of relevance feedback. We formally frame this tradeoff as a problem of optimizing the diversification of search results since relevance judgments on more diversified results have been shown to be more useful for relevance feedback. We propose a machine learning approach to adaptively optimizing the diversification of search results for each query so as to optimize the overall utility in an entire session. Experiment results on three representative retrieval test collections show that the proposed learning approach can effectively optimize the exploration---exploitation tradeoff and outperforms the traditional relevance feedback approach which only does exploitation without exploration.""], ['Integer linear programming for Constrained Multi-Aspect Committee Review Assignment', 'Automatic review assignment can significantly improve the productivity of many people such as conference organizers, journal editors and grant administrators. A general setup of the review assignment problem involves assigning a set of reviewers on a committee to a set of documents to be reviewed under the constraint of review quota so that the reviewers assigned to a document can collectively cover multiple topic aspects of the document. No previous work has addressed such a setup of committee review assignments while also considering matching multiple aspects of topics and expertise. In this paper, we tackle the problem of committee review assignment with multi-aspect expertise matching by casting it as an integer linear programming problem. The proposed algorithm can naturally accommodate any probabilistic or deterministic method for modeling multiple aspects to automate committee review assignments. Evaluation using a multi-aspect review assignment test set constructed using ACM SIGIR publications shows that the proposed algorithm is effective and efficient for committee review assignments based on multi-aspect expertise matching.'], ['Axiomatic analysis of translation language model for information retrieval', 'Statistical translation models have been shown to outperform simple document language models which rely on exact matching of words in the query and documents. A main challenge in applying translation models to ad hoc information retrieval is to estimate a translation model without training data. In this paper, we perform axiomatic analysis of translation language model for retrieval in order to gain insights about how to optimize the estimation of translation probabilities. We propose a set of constraints that a reasonable translation language model should satisfy. We check these constraints on the state-of-the-art translation estimation method based on Mutual Information and find that it does not satisfy most of the constraints. We then propose a new estimation method that better satisfies the defined constraints. Experimental results on representative TREC data sets show that the proposed new estimation method outperforms the existing Mutual Information-based estimation, suggesting that the proposed constraints are indeed helpful for designing better estimation methods for translation language model.'], ['Improving retrieval accuracy of difficult queries through generalizing negative document language models', 'When a query topic is difficult and the search results are very poor, negative feedback is a very useful method to improve the retrieval accuracy and user experience. One challenge in negative feedback is that negative documents tend to be distracting in different ways, thus as training examples, negative examples are sparse. In this paper, we solve the problem of data sparseness in the language modeling framework. We propose an optimization framework, in which we learn from a few top-ranked non-relevant examples, and search in a large space of all language models to build a more general negative language model. This general negative language model has more power in pruning the non-relevant documents, thus potentially improving the performance for difficult queries. Experiment results on representative TREC collections show that the proposed optimization framework can improve negative feedback performance over the state-of-the-art negative feedback method through generalizing negative language models.'], ['A stochastic learning-to-rank algorithm and its application to contextual advertising', 'This paper is concerned with the problem of learning a model to rank objects (Web pages, ads and etc.). We propose a framework where the ranking model is both optimized and evaluated using the same information retrieval measures such as Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP). The main difficulty in direct optimization of NDCG and MAP is that these measures depend on the rank of objects and are not differentiable. Most learning-to-rank methods that attempt to optimize NDCG or MAP approximate such measures so that they can be differentiable. In this paper, we propose a simple yet effective stochastic optimization algorithm to directly minimize any loss function, which can be defined on NDCG or MAP for the learning-to-rank problem. The algorithm employs Simulated Annealing along with Simplex method for its parameter search and finds the global optimal parameters. Experiment results using NDCG-Annealing algorithm, an instance of the proposed algorithm, on LETOR benchmark data sets show that the proposed algorithm is both effective and stable when compared to the baselines provided in LETOR 3.0. In addition, we applied the algorithm for ranking ads in contextual advertising. Our method has shown to significantly improve relevance in offline evaluation and business metrics in online tests in a real large-scale advertising serving system. To scale our computations, we parallelize the algorithm in a MapReduce framework running on Hadoop.'], ['Exploration-exploitation tradeoff in interactive relevance feedback', ""We study an interesting optimization problem in interactive feedback that aims at optimizing the tradeoff between presenting search results with the highest immediate utility to a user (but not necessarily most useful for collecting feedback information) and presenting search results with the best potential for collecting useful feedback information (but not necessarily the most useful documents from a user's perspective). Optimizing such an exploration-exploitation tradeoff is key to the optimization of the overall utility of relevance feedback to a user in the entire session of relevance feedback. We frame this tradeoff as a problem of optimizing the diversification of search results. We propose a machine learning approach to adaptively optimizing the diversification of search results for each query so as to optimize the overall utility in an entire session. Experiment results show that the proposed learning approach can effectively optimize the exploration-exploitation tradeoff and outperforms the traditional relevance feedback approach which only does exploitation without exploration.""], ['Estimation of statistical translation models based on mutual information for ad hoc information retrieval', 'As a principled approach to capturing semantic relations of words in information retrieval, statistical translation models have been shown to outperform simple document language models which rely on exact matching of words in the query and documents. A main challenge in applying translation models to ad hoc information retrieval is to estimate a translation model without training data. Existing work has relied on training on synthetic queries generated based on a document collection. However, this method is computationally expensive and does not have a good coverage of query words. In this paper, we propose an alternative way to estimate a translation model based on normalized mutual information between words, which is less computationally expensive and has better coverage of query words than the synthetic query method of estimation. We also propose to regularize estimated translation probabilities to ensure sufficient probability mass for self-translation. Experiment results show that the proposed mutual information-based estimation method is not only more efficient, but also more effective than the synthetic query-based method, and it can be combined with pseudo-relevance feedback to further improve retrieval accuracy. The results also show that the proposed regularization strategy is effective and can improve retrieval accuracy for both synthetic query-based estimation and mutual information-based estimation.'], ['Constrained multi-aspect expertise matching for committee review assignment', 'Automatic review assignment can significantly improve the productivity of many people such as conference organizers, journal editors and grant administrators. Most previous works have set the problem up as using a paper as a query to independently ""retrieve"" a set of reviewers that should review the paper. A more appropriate formulation of the problem would be to simultaneously optimize the assignments of all the papers to an entire committee of reviewers under constraints such as the review quota. In this paper, we solve the problem of committee review assignment with multi-aspect expertise matching by casting it as an integer linear programming problem. The proposed algorithm can naturally accommodate any probabilistic or deterministic method for modeling multiple aspects to automate committee review assignments. Evaluation using an existing data set shows that the proposed algorithm is effective for committee review assignments based on multi-aspect expertise matching.'], ['Enhancing Expert Finding Using Organizational Hierarchies', 'The task in expert finding is to identify members of an organization with relevant expertise on a given topic. In existing expert finding systems, profiles are constructed from sources such as email or documents, and used as the basis for expert identification. In this paper, we leverage the organizational hierarchy (depicting relationships between managers, subordinates, and peers) to find members for whom we have little or no information. We propose an algorithm to improve expert finding performance by considering not only the expertise of the member, but also the expertise of his or her neighbors. We show that providing this additional information to an expert finding system improves its retrieval performance.'], ['Multi-aspect expertise matching for review assignment', 'Review assignment is a common task that many people such as conference organizers, journal editors, and grant administrators would have to do routinely. As a computational problem, it involves matching a set of candidate reviewers with a paper or proposal to be reviewed. A common deficiency of all existing work on solving this problem is that they do not consider the multiple aspects of topics or expertise and all match the entire document to be reviewed with the overall expertise of a reviewer. As a result, if a document contains multiple subtopics, which often happens, existing methods would not attempt to assign reviewers to cover all the subtopics; instead, it is quite possible that all the assigned reviewers would cover the major subtopic quite well, but not covering any other subtopic. In this paper, we study how to model multiple aspects of expertise and assign reviewers so that they together can cover all subtopics in the document well. We propose three general strategies for solving this problem and propose new evaluation measures for this task. We also create a multi-aspect review assignment test set using ACM SIGIR publications. Experiment results on this data set show that the proposed methods are effective for assigning reviewers to cover all topical aspects of a document.']]"
Fabio Massimo Zanzotto,"[['Viewpoint: human-in-the-loop artificial intelligence', 'Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet.In this paper, we propose Human-in-the-loop Artificial Intelligence (HitAI) as a fairer paradigm for AI systems. Recognizing that any AI system has humans in the loop, HitAI will reward these aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Merry Men, HitAI researchers should fight for a fairer Robin Hood Artificial Intelligence that gives back what it steals.'], ['Have You Lost the Thread? Discovering Ongoing Conversations in Scattered Dialog Blocks', 'Finding threads in textual dialogs is emerging as a need to better organize stored knowledge. We capture this need by introducing the novel task of discovering ongoing conversations in scattered dialog blocks. Our aim in this article is twofold. First, we propose a publicly available testbed for the task by solving the insurmountable problem of privacy of Big Personal Data. In fact, we showed that personal dialogs can be surrogated with theatrical plays. Second, we propose a suite of computationally light learning models that can use syntactic and semantic features. With this suite, we showed that models for this challenging task should include features capturing shifts in language use and, possibly, modeling underlying scripts.'], ['Predicting embedded syntactic structures from natural language sentences with neural network approaches', 'Syntactic parsing is a key component of natural language understanding and, traditionally, has a symbolic output. Recently, a new approach for predicting syntactic structures from sentences has emerged: directly producing small and expressive vectors that embed in syntactic structures. In this approach, parsing produces distributed representations. In this paper, we advance the frontier of these novel predictors by using the learning capabilities of neural networks. We propose two approaches for predicting the embedded syntactic structures. The first approach is based on a multi-layer perceptron to learn how to map vectors representing sentences into embedded syntactic structures. The second approach exploits recurrent neural networks with long short-term memory (LSTM-RNN-DRP) to directly map sentences to these embedded structures. We show that both approaches successfully exploit word information to learn syntactic predictors and achieve a significant performance advantage over previous methods. Results on the Penn Treebank corpus are promising. With the LSTM-RNN-DRP, we improve the previous state-of-the-art method by 8.68%.'], ['Decoding Distributed Tree Structures', 'Encoding structural information in low-dimensional vectors is a recent trend in natural language processing that builds on distributed representations [14]. However, although the success in replacing structural information in final tasks, it is still unclear whether these distributed representations contain enough information on original structures. In this paper we want to take a specific example of a distributed representation, the distributed trees DT [17], and analyze the reverse problem: can the original structure be reconstructed given only its distributed representation? Our experiments show that this is indeed the case, DT can encode a great deal of information of the original tree, and this information is often enough to reconstruct the original object format.'], ['When the whole is not greater than the combination of its parts: A ""decompositional"" look at compositional distributional semantics', 'Distributional semantics has been extended to phrases and sentences by means of composition operations. We look at how these operations affect similarity measurements, showing that similarity equations of an important class of composition methods can be decomposed into operations performed on the subparts of the input phrases. This establishes a strong link between these models and convolution kernels.'], ['Linear Online Learning over Structured Data with Distributed Tree Kernels', 'Online algorithms are an important class of learning machines as they are extremely simple and computationally efficient. Kernel methods versions can handle structured data, such as trees, and achieve state-of-the-art performance. However kernelized versions of Online Learning algorithms slow down when the number of support vectors becomes large. The traditional way to cope with this problem is introducing budgets that set the maximum number of support vectors. In this paper, we investigate Distributed Trees (DT) as an efficient way to use structured data in online learning. DTs effectively embed the huge feature space of the tree fragments into small vectors, so enabling the use of linear versions of kernel machines over tree structured data. We experiment with the Passive-Aggressive (PA) algorithm by comparing the linear and the kernelized version. A massive dataset made with tree structured data is employed: it is originated from a natural language processing task, the Boundary Detection in the context of Semantic Role Labeling over Frame Net. Results on a sample of the final data show that the DTs along with the Linear PA algorithm and the Tree Kernel along with the Bundgeted PA achieve comparable results in terms of f1-measure. Finally, the exploration of the full dataset allows the former to improve the performance on the classification task, with respect to the latter.'], ['Travel with Words: An Innovative Vision on Travelling', 'Modern travelers wish to explore the world at their own pace, following their own expectations and cultural interests. The travel industry should adapt to this new market environment. A large part of cultural travels is based on history and historical events, and books are a near endless repository of such facts. Moreover, novels are a friendlier gateway to culture than handbooks or reference books. And readers often grow the necessity to see, feel, and taste the imaginary world they are reading about in their own language. By leveraging on books and on modern information and communication technologies, we envision a new way to define travels based on the ""Cultural Travel Clouds"". This paper discusses this vision.'], ['Parallels between machine and brain decoding', 'We report some existing work, inspired by analogies between human thought and machine computation, showing that the informational state of a digital computer can be decoded in a similar way to brain decoding. We then discuss some proposed work that would leverage this analogy to shed light on the amount of information that may be missed by the technical limitations of current neuroimaging technologies.'], ['Distributed tree kernels', 'In this paper, we propose the distributed tree kernels (DTK) as a novel method to reduce time and space complexity of tree kernels. Using a linear complexity algorithm to compute vectors for trees, we embed feature spaces of tree fragments in low-dimensional spaces where the kernel computation is directly done with dot product. We show that DTKs are faster, correlate with tree kernels, and obtain a statistically similar performance in two natural language processing tasks.'], ['Linguistic redundancy in Twitter', 'In the last few years, the interest of the research community in micro-blogs and social media services, such as Twitter, is growing exponentially. Yet, so far not much attention has been paid on a key characteristic of micro-blogs: the high level of information redundancy. The aim of this paper is to systematically approach this problem by providing an operational definition of redundancy. We cast redundancy in the framework of Textual Entailment Recognition. We also provide quantitative evidence on the pervasiveness of redundancy in Twitter, and describe a dataset of redundancy-annotated tweets. Finally, we present a general purpose system for identifying redundant tweets. An extensive quantitative evaluation shows that our system successfully solves the redundancy detection task, improving over baseline systems with statistical significance.'], ['Distributed structures and distributional meaning', 'Stemming from distributed representation theories, we investigate the interaction between distributed structure and distributional meaning. We propose a pure distributed tree (DT) and distributional distributed tree (DDT). DTs and DDTs are exploited for defining distributed tree kernels (DTKs) and distributional distributed tree kernels (DDTKs). We compare DTKs and DDTKs in two tasks: approximating tree kernels TK (Collins and Duffy, 2002); performing textual entailment recognition (RTE). Results show that DTKs correlate with TKs and perform in RTE better than DDTKs. Then, including distributional vectors in distributed structures is a very difficult task.'], ['Efficient Graph Kernels for Textual Entailment Recognition', 'One of the most important research area in Natural Language Processing concerns the modeling of semantics expressed in text. Since foundational work in Natural Language Understanding has shown that a deep semantic approach is still not feasible, current research is focused on shallow methods combining linguistic models and machine learning techniques. The latter aim at learning semantic models, like those that can detect the entailment between the meaning of two text fragments, by means of training examples described by specific features. These are rather difficult to design since there is no linguistic model that can effectively encode the lexico-syntactic level of a sentence and its corresponding semantic models. Thus, the adopted solution consists in exhaustively describing training examples by means of all possible combinations of sentence words and syntactic information. The latter, typically expressed as parse trees of text fragments, is often encoded in the learning process using graph algorithms. In this paper, we propose a class of graphs, the tripartite directed acyclic graphs (tDAGs), which can be efficiently used to design algorithms for graph kernels for semantic natural language tasks involving sentence pairs. These model the matching between two pairs of syntactic trees in terms of all possible graph fragments. Interestingly, since tDAGs encode the association between identical or similar words (i.e. variables), it can be used to represent and learn first-order rules, i.e. rules describable by first-order logic. We prove that our matching function is a valid kernel and we empirically show that, although its evaluation is still exponential in the worst case, it is extremely efficient and more accurate than the previously proposed kernels.'], ['Inductive probabilistic taxonomy learning using singular value decomposition', 'Capturing word meaning is one of the challenges of natural language processing (NLP). Formal models of meaning, such as networks of words or concepts, are knowledge repositories used in a variety of applications. To be effectively used, these networks have to be large or, at least, adapted to specific domains. Learning word meaning from texts is then an active area of research. Lexico-syntactic pattern methods are one of the possible solutions. Yet, these models do not use structural properties of target semantic relations, e.g. transitivity, during learning. In this paper, we propose a novel lexico-syntactic pattern probabilistic method for learning taxonomies that explicitly models transitivity and naturally exploits vector space model techniques for reducing space dimensions. We define two probabilistic models: the direct probabilistic model and the induced probabilistic model. The first is directly estimated on observations over text collections. The second uses transitivity on the direct probabilistic model to induce probabilities of derived events. Within our probabilistic model, we also propose a novel way of using singular value decomposition as unsupervised method for feature selection in estimating direct probabilities. We empirically show that the induced probabilistic taxonomy learning model outperforms state-of-the-art probabilistic models and our unsupervised feature selection method improves performance.'], ['Comparing EEG/ERP-like and fMRI-like techniques for reading machine thoughts', 'fMRI and ERP/EEG are two different sources for scanning the brain for building mind state decoders. fMRI produces accurate images but it is expensive and cumbersome. ERP/EEG is cheaper and potentially wearable but it gives more coarse-grain data. Recently the metaphor between machines and brains has been introduced in the context of mind state decoders: the ""readers for machines\' thoughts"". This metaphor gives the possibility for comparing mind state decoder methods in a more controlled setting. In this paper, we compare the fMRI and ERP/EEG in the context of building ""readers for machines\' thoughts"". We want assess if the cheaper ERP/EEG can be competitive with fMRI models for building decoders for mind states. Experiments show that accuracy of ""readers"" based on ERP/EEG-like data are considerably lower than the one of those based on fMRI-like images.'], ['Estimating linear models for compositional distributional semantics', 'In distributional semantics studies, there is a growing attention in compositionally determining the distributional meaning of word sequences. Yet, compositional distributional models depend on a large set of parameters that have not been explored. In this paper we propose a novel approach to estimate parameters for a class of compositional distributional models: the additive models. Our approach leverages on two main ideas. Firstly, a novel idea for extracting compositional distributional semantics examples. Secondly, an estimation method based on regression models for multiple dependent variables. Experiments demonstrate that our approach outperforms existing methods for determining a good model for compositional distributional semantics.'], ['Syntactic/semantic structures for textual entailment recognition', 'In this paper, we describe an approach based on off-the-shelf parsers and semantic resources for the Recognizing Textual Entailment (RTE) challenge that can be generally applied to any domain. Syntax is exploited by means of tree kernels whereas lexical semantics is derived from heterogeneous resources, e.g. WordNet or distributional semantics through Wikipedia. The joint syntactic/semantic model is realized by means of tree kernels, which can exploit lexical related-ness to match syntactically similar structures, i.e. whose lexical compounds are related. The comparative experiments across different RTE challenges and traditional systems show that our approach consistently and meaningfully achieves high accuracy, without requiring any adaptation or tuning.'], ['Probabilistic Ontology Learner in Semantic Turkey', 'In this paper we present the Semantic Turkey Ontology Learner (ST-OL), an incremental ontology learning system, that follows two main ideas: (1) putting final users in the learning loop; (2) using a probabilistic ontology learning model that exploits transitive relations for inducing better extraction models.'], ['Reading what machines ""think""', 'In this paper, we want to farther advance the parallelism between models of the brain and computing machines. We want to apply the same idea underlying neuroimaging techniques to electronic computers. Applying this parallelism, we can address these two questions: (1) how far we can go with neuroimaging in understanding human mind? (foundational perspective); (2) can we understand what computers ""think""? (applicative perspective). Our experiments demonstrate that it is possible to believe that both questions have positive answers.'], ['A machine learning approach to textual entailment recognition', 'Designing models for learning textual entailment recognizers from annotated examples is not an easy task, as it requires modeling the semantic relations and interactions involved between two pairs of text fragments. In this paper, we approach the problem by first introducing the class of pair feature spaces, which allow supervised machine learning algorithms to derive first-order rewrite rules from annotated examples. In particular, we propose syntactic and shallow semantic feature spaces, and compare them to standard ones. Extensive experiments demonstrate that our proposed spaces learn first-order derivations, while standard ones are not expressive enough to do so.'], ['Efficient kernels for sentence pair classification', 'In this paper, we propose a novel class of graphs, the tripartite directed acyclic graphs (tDAGs), to model first-order rule feature spaces for sentence pair classification. We introduce a novel algorithm for computing the similarity in first-order rewrite rule feature spaces. Our algorithm is extremely efficient and, as it computes the similarity of instances that can be represented in explicit feature spaces, it is a valid kernel function.'], ['SVD feature selection for probabilistic taxonomy learning', 'In this paper, we propose a novel way to include unsupervised feature selection methods in probabilistic taxonomy learning models. We leverage on the computation of logistic regression to exploit unsupervised feature selection of singular value decomposition (SVD). Experiments show that this way of using SVD for feature selection positively affects performances.'], ['Natural Language Processing Across Time: An Empirical Investigation on Italian', 'In this paper, we study how existing natural language processing tools for Italian perform on ancient texts. The first goal is to understand to what extent such tools can be used ""as they are"" for the automatic analysis of old literary works. Indeed, while NLP tools for Italian achieve today good performance, it is not clear if they could be successfully used for the humanities, to support the critical study of historical works. Our analysis will show how tools\' performance systematically vary across different time periods, and within literary movements. As a second goal, we want to verify whether or not simple customization methods can improve the tools performance over the old works.'], ['Encoding tree pair-based graphs in learning algorithms: the textual entailment recognition case', 'In this paper, we provide a statistical machine learning representation of textual entailment via syntactic graphs constituted by tree pairs. We show that the natural way of representing the syntactic relations between text and hypothesis consists in the huge feature space of all possible syntactic tree fragment pairs, which can only be managed using kernel methods. Experiments with Support Vector Machines and our new kernels for paired trees show the validity of our interpretation.'], ['Shallow semantics in fast textual entailment rule learners', 'In this paper, we briefly describe two enhancements of the cross-pair similarity model for learning textual entailment rules: 1) the typed anchors and 2) a faster computation of the similarity. We will report and comment on the preliminary experiments and on the submission results.'], ['Fast and effective kernels for relational learning from texts', 'In this paper, we define a family of syntactic kernels for automatic relational learning from pairs of natural language sentences. We provide an efficient computation of such models by optimizing the dynamic programming algorithm of the kernel evaluation. Experiments with Support Vector Machines and the above kernels show the effectiveness and efficiency of our approach on two very important natural language tasks, Textual Entailment Recognition and Question Answering.'], ['Experimenting a ""General purpose"" textual entailment learner in AVE', 'In this paper we present the use of a ""general purpose"" textual entailment recognizer in the Answer Validation Exercise (AVE) task. Our system is designed to learn entailment rules from annotated examples. Its main feature is the use of Support Vector Machines (SVMs) with kernel functions based on cross-pair similarity between entailment pairs. We experimented with our system using different training sets: RTE and AVE data sets. The comparative results show that entailment rules can be learned. Although, the high variability of the outcome prevents us to derive definitive conclusions, the results show that our approach is quite promising and improvable in the future.'], ['Discovering asymmetric entailment relations between verbs using selectional preferences', 'In this paper we investigate a novel method to detect asymmetric entailment relations between verbs. Our starting point is the idea that some point-wise verb selectional preferences carry relevant semantic information. Experiments using Word-Net as a gold standard show promising results. Where applicable, our method, used in combination with other approaches, significantly increases the performance of entailment detection. A combined approach including our model improves the AROC of 5% absolute points with respect to standard models.'], ['Automatic learning of textual entailments with cross-pair similarities', 'In this paper we define a novel similarity measure between examples of textual entailments and we use it as a kernel function in Support Vector Machines (SVMs). This allows us to automatically learn the rewrite rules that describe a non trivial set of entailment cases. The experiments with the data sets of the RTE 2005 challenge show an improvement of 4.4% over the state-of-the-art methods.'], ['Discovering verb relations in corpora: distributional versus non-distributional approaches', 'Verbs represent a way in which ontological relationships between concepts and instances are expressed in natural language utterances. Moreover, an organized network of semantically related verbs can play a crucial role in applications. For example, if a Question-Answering system could exploit the direction of the entailment relation win → play, it may expand the question “Who played against Liverpool?” with “X won against Liverpool” and it may avoid the expansion of “Who won against Liverpool?” in “X played against Liverpool” that would be wrong. In this paper, we present a survey of the methods proposed to extract verb relations in corpora. These methods can be divided in two classes: those using the Harris distributional hypothesis and those based on point-wise assertions. These methods are analysed and compared.'], ['Similarity between pairs of co-indexed trees for textual entailment recognition', 'In this paper we present a novel similarity between pairs of co-indexed trees to automatically learn textual entailment classifiers. We defined a kernel function based on this similarity along with a more classical intra-pair similarity. Experiments show an improvement of 4.4 absolute percent points over state-of-the-art methods.'], ['A linguistic inspection of textual entailment', 'Recognition of textual entailment is not an easy task. In fact, early experimental evidences in [1] seems to demonstrate that even human judges often fail in reaching an agreement on the existence of entailment relation between two expressions. We aim to contribute to the theoretical and practical setting of textual entailment, through both a linguistic inspection of the textual entailment phenomenon and the description of a new promising approach to recognition, as implemented in the system we proposed at the RTE competition [2].'], ['Discovering entailment relations using ""textual entailment patterns""', 'In this work we investigate methods to enable the detection of a specific type of textual entailment (strict entailment), starting from the preliminary assumption that these relations are often clearly expressed in texts. Our method is a statistical approach based on what we call textual entailment patterns, prototypical sentences hiding entailment relations among two activities. We experimented the proposed method using the entailment relations of WordNet as test case and the web as corpus where to estimate the probabilities; obtained results will be shown.'], ['AI/NLP technologies applied to spacecraft mission design', 'In this paper we propose the model of a prototypical NLP architecture of an information access system to support a team of experts in a scientific design task, in a shared and heterogeneous framework. Specifically, we believe AI/NLP can be helpful in several tasks, such as the extraction of implicit information needs enclosed in meeting minutes or other documents, analysis of explicit information needs expressed through Natural Language, processing and indexing of document collections, extraction of required information from documents, modeling of a common knowledge base, and, finally, identification of important concepts through the automatic extraction of terms. In particular, we envisioned this architecture in the specific and practical scenario of the Concurrent Design Facility (CDF) of the European Space Agency (ESA), in the framework of the SHUMI project (Support To HUman Machine Interaction) developed in collaboration with the ESA/ESTEC - ACT (Advanced Concept Team).'], ['Learning textual entailment on a distance feature space', 'Textual Entailment recognition is a very difficult task as it is one of the fundamental problems in any semantic theory of natural language. As in many other NLP tasks, Machine Learning may offer important tools to better understand the problem. In this paper, we will investigate the usefulness of Machine Learning algorithms to address an apparently simple and well defined classification problem: the recognition of Textual Entailment. Due to its specificity, we propose an original feature space, the distance feature space, where we model the distance between the elements of the candidate entailment pairs. The method has been tested on the data of the Recognizing Textual Entailment (RTE) Challenge.'], ['Understanding the Web through its Language', 'A tight integration between ontological and linguistic knowledge is critical within the information processes of the Semantic Web. In Information Extraction, ontologies should include knowledge components neglected in domain conceptualizations generally used for other tasks. In this paper, we analyze such critical information in the light of existing applications. Accordingly, a methodology for semi-automatic development of an IE ontology integrating pre-existing domain and lexical knowledge is presented. The proposed ontological framework supports the discovery of new relations among known concepts by means of text processing, but also induction of new conceptual information.'], ['Identifying relational concept lexicalisations by using general linguistic knowledge', 'This paper analyses how general-purpose semantic hierarchies could be helpful in the construction of one-to-many mappings between the coarse-grained relational concepts and the corresponding linguistic realisations. We propose an original model, the semantic fingerprint, for exploiting ambiguous semantic information within the feature vector model.'], ['Modelling Semantic Grid Knowledge Embedded in Documents', 'The growing success of Grid technologies inside scientific communities has produced an increasing need for the development of tools and methodologies able to support knowledge sharing and handling among people, built upon the Grid. This ""semantic"" infrastructure is becoming to be referred as Semantic Grid. In this paper we propose an original approach to the development of a system for the creation of the Knowledge Layer of the Semantic Grid, that is, the layer which carries the informative content that the community shares. Using well-assessed Natural Language Processing and Machine Learning methodologies and techniques, our goal is to acquire and organize the information stored in the Grid, where this information is supposed to be represented in unstructured documents. Our intent is to extract and shape knowledge in syntactic patterns and organize them into a hierarchy of relational concepts, whose goal is to improve the process of knowledge retrieval and maintenance.'], ['Integrating Ontological and Linguistic Knowledge for Conceptual Information Extraction', 'Text understanding makes strong assumptions about the conceptualisation of the underlying knowledge domain. This mediates between the accomplishment of the specific task at the one hand and the knowledge expressed in the target text fragments at the other. However, building domain conceptualisations from scratch is a very complex and time-consuming task. Traditionally, the re-use of available domain resources, although not constituting always the best, has been applied as an accurate and cost effective solution.In this paper, we investigate the possibility of exploiting sources of domain knowledge (e.g. a subject reference system) to build a linguistically motivated domain concept hierarchy. The limitation connected with the use of domain taxonomies as ontological resources will be firstly discussed in the specific light of IE, i.e. for supporting linguistic inference. We then define a method for integrating the taxonomical domain knowledge and a general-purpose lexical knowledge base, like WordNet. A case study, i.e. the integration of the MeSH, Medical Subject Headings, and Word-Net, will be then presented as a proof of the effectiveness and accuracy of the overall approach.'], ['Personalizing Web Publishing via Information Extraction', 'The need for quick and personalized access to distributed information is currently amplified by the growing capabilities of fast and global computer and telecommunications networks. As this empowers the penetration of more sophisticated information management practice, it also increases the complexity of the required information technologies. Multimedia applications for Web-based information access provide high-level interfaces and facilities but still rely on poorly harmonized multimodal front ends. Search engines, for example, are still tied to traditional keyword-based methods and depend loosely on Web content. The authors are convinced that more emphasis on the linguistic content is a viable path to intelligent Web services. Textual information can provide a logical approach to Web authoring where hypertext links can be inferred from conceptual information extracted from the texts. They have pursued these ideas in Namic, a multilingual text classification and hyperlinking system based an a knowledge-based approach to information extraction. This article describes the Namic architecture and its application scenario.'], ['Knowledge-based multilingual document analysis', 'The growing availability of multilingual resources, like EuroWordnet, has recently inspired the development of large scale linguistic technologies, e.g. multilingual IE and Q&A, that were considered infeasible until a few years ago. In this paper a system for categorisation and automatic authoring of news streams in different languages is presented. In our system, a knowledge-based approach to Information Extraction is adopted as a support for hyperlinking. Authoring across documents in different languages is triggered by Named Entities and event recognition. The matching of events in texts is carried out by discourse processing driven by a large scale world model. This kind of multilingual analysis relies on a lexical knowledge base of nouns(i.e. the EuroWordnet Base Concepts) shared among English, Spanish and Italian lexicons. The impact of the design choices on the language independence and the possibilities it opens for automatic learning of the event hierarchy will be discussed.'], ['Decision trees as explicit domain term definitions', 'Terminology Acquisition (TA) methods are viable solutions for the knowledge bottleneck problem that confines knowledge-intensive information access systems (such as Information Extraction systems) to restricted application scenarios. TA can be seen as a way to inspect large text collections for extracting concise domain knowledge. In this paper we argue that major insights over the notion of term can be obtained by investigating a more domain-based term definition. We propose a decision tree learning approach as an interesting model of the human TA activity. An incremental model is proposed to study the evolution of the term definition during the TA process over a particular implicit domain model. The experimental apparatus is based on robust text processing tools that support a large scale investigation. The good results suggest that the proposed automatic TA model can support the development of conceptual domain dictionaries as required by knowledge-based information systems.'], ['Acquisition of domain conceptual dictionaries via decision tree learning', 'Knowledge based systems usually rely on large size domain models needed to support reasoning and decision-making. The development of realistic models represents a critical and labour intensive phase. Automatic terminology acquisition (TA) has been proposed as the task of automatically extracting specialized dictionaries from raw texts useful for application purposes like precise information retrieval and machine translation. In this paper we argue that TA provides a significant contribution in the development of ontological components of a knowledge bases. We therefore propose an automatic knowledge acquisition architecture for the TA process based on robust methods for text processing and on algorithms for learning decision trees. An incremental semiautomatic approach is proposed to enable the first steps in the development of a domain ontology. The novel aspects of the method rely on the use of syntagmatic and lexical properties of terms combined with analogous (negative) evidences observable for non-terms. The underlying assumptions as well as the different adopted linguistic representations have been extensively investigated over a large test set. The scale of the target test data provides empirical evidence of the superiority of the method over more quantitative approaches. The proposed architecture is thus a viable approach to the development of conceptual domain dictionaries.'], ['Parsing engineering and empirical robustness', 'Robustness has been traditionally stressed as a general desirable property of any computational model and system. The human NL interpretation device exhibits this property as the ability to deal with odd sentences. However, the difficulties in a theoretical explanation of robustness within the linguistic modelling suggested the adoption of an empirical notion. In this paper, we propose an empirical definition of robustness based on the notion of performance. Furthermore, a framework for controlling the parser robustness in the design phase is presented. The control is achieved via the adoption of two principles: the modularisation, typical of the software engineering practice, and the availability of domain adaptable components. The methodology has been adopted for the production of CHAOS, a pool of syntactic modules, which has been used in real applications. This pool of modules enables a large validation of the notion of empirical robustness, on the one side, and of the design methodology, on the other side, over different corpora and two different languages (English and Italian).'], ['Flexible Parsing Architectures for NLP Applications', 'The requirements of different NLP applications have strong implications on the design and implementation of the related syntactic recognisers. In this paper, a fine-grained modular parser design framework is presented. Our aim is to reduce the design of a parsing processors to the composition of a pool of basic modules. Results over sample parsers and criteria for optimising coverage and accuracy are discussed.'], ['Multilingual authoring: the NAMIC approach', 'With increasing amounts of electronic information available, and the increase in the variety of languages used to produce documents of the same type, the problem of how to manage similar documents in different languages arises. This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be effectively carried out. This work, funded by the European Union, is applied to the Multilingual Authoring of news agency text. We have applied methods from Natural Language Processing, especially Information Extraction technology, to both monolingual and Multilingual Authoring.']]"
Stefano Serra-Capizzano,"[['Exact formulae and matrix-less eigensolvers for block banded symmetric Toeplitz matrices', 'AbstractPrecise asymptotic expansions for the eigenvalues of a Toeplitz matrix $$T_n(f)$$, as the matrix size n tends to infinity, have recently been obtained, under suitable assumptions on the associated generating function f. A restriction is that f has to be polynomial, monotone, and scalar-valued. In this paper we focus on the case where $$\\mathbf {f}$$ is an $$s\\times s$$ matrix-valued trigonometric polynomial with $$s\\ge 1$$, and $$T_n(\\mathbf {f})$$ is the block Toeplitz matrix generated by $$\\mathbf {f}$$, whose size is $$N(n,s)=sn$$. The case $$s=1$$ corresponds to that already treated in the literature. We numerically derive conditions which ensure the existence of an asymptotic expansion for the eigenvalues. Such conditions generalize those known for the scalar-valued setting. Furthermore, following a proposal in the scalar-valued case by the first author, Garoni, and the third author, we devise an extrapolation algorithm for computing the eigenvalues of banded symmetric block Toeplitz matrices with a high level of accuracy and a low computational cost. The resulting algorithm is an eigensolver that does not need to store the original matrix, does not need to perform matrix-vector products, and for this reason is called matrix-less. We use the asymptotic expansion for the efficient computation of the spectrum of special block Toeplitz structures and we provide exact formulae for the eigenvalues of the matrices coming from the $$\\mathbb {Q}_p$$ Lagrangian Finite Element approximation of a second order elliptic differential problem. Numerical results are presented and critically discussed.'], ['Spectral analysis of finite-dimensional approximations of 1d waves in non-uniform grids', 'We study the gap of discrete spectra of the Laplace operator in 1d for non-uniform meshes by analyzing the corresponding spectral symbol, which allows to show how to design the discretization grid for improving the gap behavior. The main tool is the study of a univariate monotonic version of the spectral symbol, obtained by employing a proper rearrangement via the GLT theory. We treat in detail the case of basic finite-difference approximations. In a second step, we pass to precise approximation schemes, coming from the celebrated Galerkin isogeometric analysis based on B-splines of degree p and global regularity $$C^{p-1}$$Cp-1, and finally we address the case of finite-elements with global regularity $$C^0$$C0 and local polynomial degree p. The surprising result is that the GLT approach allows a unified spectral treatment of the various schemes also in terms of the preservation of the average gap property, which is necessary for the uniform gap property. The analytical results are illustrated by a number of numerical experiments. We conclude by discussing some open problems.'], ['Are the eigenvalues of preconditioned banded symmetric Toeplitz matrices known in almost closed form?', 'Bogoya, Böttcher, Grudsky, and Maximenko have recently obtained the precise asymptotic expansion for the eigenvalues of a sequence of Toeplitz matrices {Tn(f)}, under suitable assumptions on the associated generating function f. In this paper, we provide numerical evidence that some of these assumptions can be relaxed and extended to the case of a sequence of preconditioned Toeplitz matrices {Tný1(g)Tn(f)}, for f trigonometric polynomial, g nonnegative, not identically zero trigonometric polynomial, r = f/g, and where the ratio r plays the same role as f in the nonpreconditioned case. Moreover, based on the eigenvalue asymptotics, we devise an extrapolation algorithm for computing the eigenvalues of preconditioned banded symmetric Toeplitz matrices with a high level of accuracy, with a relatively low computational cost, and with potential application to the computation of the spectrum of differential operators.'], ['Uniform Convergence of V-cycle Multigrid Algorithms for Two-Dimensional Fractional Feynman---Kac Equation', 'When solving large linear systems stemming from the approximation of elliptic partial differential equations (PDEs), it is known that the V-cycle multigrid method (MGM) can significantly lower the computational cost. Many convergence estimates already exist for the V-cycle MGM: for example, using the regularity or approximation assumptions of the elliptic PDEs, the results are obtained in Bank and Douglas (SIAM J Numer Anal 22:617---633, 1985), Bramble and Pasciak (Math Comp 49:311---329, 1987); in the case of multilevel matrix algebras (like circulant, tau, Hartely) (Aric et al. in SIAM J Matrix Anal Appl 26:186---214, 2004; Aric and Donatelli in Numer Math 105:511---547, 2007), special prolongation operators are provided and the related convergence results are rigorously developed, using a functional approach. In this paper we derive new uniform convergence estimates for the V-cycle MGM applied to symmetric positive definite Toeplitz block tridiagonal matrices, by also discussing few connections with previous results. More concretely, the contributions of this paper are as follows: (1) It tackles the Toeplitz systems directly for the elliptic PDEs. (2) Simple (traditional) restriction operator and prolongation operator are employed in order to handle general Toeplitz systems at each level of the recursion. Such a technique is then applied to systems of algebraic equations generated by the difference scheme of the two-dimensional fractional Feynman---Kac equation, which describes the joint probability density function of non-Brownian motion. In particular, we consider the two coarsening strategies, i.e., doubling the mesh size (geometric MGM) and Galerkin approach (algebraic MGM), which lead to the distinct coarsening stiffness matrices in the general case: however, several numerical experiments show that the two algorithms produce almost the same error behaviour.'], ['Higher order derivative-free iterative methods with and without memory for systems of nonlinear equations', 'A derivative-free family of iterations without memory consisting of three steps for solving nonlinear systems of equations is brought forward. Then, the main aim of the paper is furnished by proposing several novel schemes with memory possessing higher rates of convergence. Analytical discussions are reported and the theoretical efficiency of the methods is studied. Application of the schemes in solving partial differential equations is finally provided to support the theoretical discussions.'], ['Function-based block multigrid strategy for a two-dimensional linear elasticity-type problem', 'We consider the solution of block-coupled large-scale linear systems of equations, arising from the finite element approximation of the linear elasticity problem. Due to the large scale of the problems we use properly preconditioned iterative methods, where the preconditioners utilize the underlying block matrix structures, involving inner block solvers and, when suited, broadly established tools such as the algebraic Multigrid method (AMG).For the considered problem, despite of its optimal rate of convergence, AMG, as implemented in some publicly available scientific libraries, imposes unacceptably high demands for computer resources. In this paper we propose and analyze an efficient multilevel preconditioner, based on the Generalized Locally Toeplitz framework, with a specialized transfer operator. We prove and numerically illustrate the optimal convergence rate of the proposed preconditioner, and experimentally report memory and CPU time savings. We also provide comparisons with respect to another aggregation-based algebraic multigrid algorithm.'], ['Multigrid methods for cubic spline solution of two point (and 2D) boundary value problems', 'In this paper we propose a scheme based on cubic splines for the solution of the second order two point boundary value problems. The solution of the algebraic system is computed by using optimized multigrid methods. In particular the transformation of the stiffness matrix essentially in a block Toeplitz matrix and its spectral analysis allow to choose smoothers able to reduce error components related to the various frequencies and to obtain an optimal method. The main advantages of our strategy can be listed as follows: (i) a fourth order of accuracy combined with a quadratic conditioning matrix, (ii) a resulting matrix structure whose eigenvalues can be compactly described by a symbol (this information is the key for designing an optimal multigrid method). Finally, some numerics that confirm the predicted behavior of the method are presented and a discussion on the two dimensional case is given, together with few 2D numerical experiments.'], ['Solving systems of nonlinear equations when the nonlinearity is expensive', 'Construction of multi-step iterative method for solving system of nonlinear equations is considered, when the nonlinearity is expensive. The proposed method is divided into a base method and multi-step part. The convergence order of the base method is five, and each step of multi-step part adds additive-factor of five in the convergence order of the base method. The general formula of convergence order is 5 ( m - 2 ) where m ( 3 ) is the step number. For a single instance of the iterative method we only compute two Jacobian and inversion of one Jacobian is required. The direct inversion of Jacobian is avoided by computing LU factors. The computed LU factors are used in the multi-step part for solving five systems of linear equations that make the method computational efficient. The distinctive feature of the underlying multi-step iterative method is the single call to the computationally expensive nonlinear function and thus offers an increment of additive-factor of five in the convergence order per single call. The numerical simulations reveal that our proposed iterative method clearly shows better performance, where the computational cost of the involved nonlinear function is higher than the computational cost for solving five lower and upper triangular systems.'], ['Spectral analysis and structure preserving preconditioners for fractional diffusion equations', 'Fractional partial order diffusion equations are a generalization of classical partial differential equations, used to model anomalous diffusion phenomena. When using the implicit Euler formula and the shifted Grünwald formula, it has been shown that the related discretizations lead to a linear system whose coefficient matrix has a Toeplitz-like structure. In this paper we focus our attention on the case of variable diffusion coefficients. Under appropriate conditions, we show that the sequence of the coefficient matrices belongs to the Generalized Locally Toeplitz class and we compute the symbol describing its asymptotic eigenvalue/singular value distribution, as the matrix size diverges. We employ the spectral information for analyzing known methods of preconditioned Krylov and multigrid type, with both positive and negative results and with a look forward to the multidimensional setting. We also propose two new tridiagonal structure preserving preconditioners to solve the resulting linear system, with Krylov methods such as CGNR and GMRES. A number of numerical examples show that our proposal is more effective than recently used circulant preconditioners.'], ['Computational evaluation of multi-iterative approaches for solving graph-structured large linear systems', 'We analyse the practical efficiency of multi-iterative techniques for the numerical solution of graph-structured large linear systems. In particular we evaluate the effectiveness of several combinations of coarser-grid operators which preserve the graph structure of the projected matrix at the inner levels and smoothers. We also discuss and evaluate some possible strategies (inverse projection and dense projection) to connect coarser-grid operators and graph-based preconditioners. Our results show that an appropriate choice of adaptive projectors and tree-based preconditioned conjugate gradient methods result in highly effective and robust approaches, that are capable to efficiently solve large-scale, difficult systems, for which the known iterative solvers alone can be rather slow.'], ['Accelerated multigrid for graph Laplacian operators', 'We consider multigrid type techniques for the numerical solution of large linear systems, whose coefficient matrices show the structure of (weighted) graph Laplacian operators. We combine ad hoc coarser-grid operators with iterative techniques used as smoothers. Empirical tests suggest that the most effective smoothers have to be of Krylov type with subgraph preconditioners, while the projectors, which define the coarser-grid operators, have to be designed for maintaining as much as possible the graph structure of the projected matrix at the inner levels. The main theoretical contribution of the paper is the characterization of necessary and sufficient conditions for preserving the graph structure. In this framework it is possible to explain why the classical projectors inherited from differential equations are good in the differential context and why they may behave unsatisfactorily for unstructured graphs. Furthermore, we report and discuss several numerical experiments, showing that our approach is effective even in very difficult cases where the known approaches are rather slow. As a conclusion, the main advantage of the proposed approach is the robustness, since our multigrid type technique behaves uniformly well in all cases, without requiring either the setting or the knowledge of critical parameters, as it happens when using the best known preconditioned Krylov methods.'], ['Higher order multi-step iterative method for computing the numerical solution of systems of nonlinear equations', 'In the present study, we consider multi-step iterative method to solve systems of nonlinear equations. Since the Jacobian evaluation and its inversion are expensive, in order to achieve a better computational efficiency, we compute Jacobian and its inverse only once in a single cycle of the proposed multi-step iterative method. Actually the involved systems of linear equations are solved by employing the LU-decomposition, rather than inversion. The primitive iterative method (termed base method) has convergence-order (CO) five and then we describe a matrix polynomial of degree two to design a multi-step method. Each inclusion of single step in the base method will increase the convergence-order by three. The general expression for CO is 3 s - 1 , where s is the number of steps of the multi-step iterative method. Computational efficiency is also addressed in comparison with other existing methods. The claimed convergence-rates proofs are established. The new contribution in this article relies essentially in the increment of CO by three for each added step, with a comparable computational cost in comparison with existing multi-steps iterative methods. Numerical assessments are made which justify the theoretical results: in particular, some systems of nonlinear equations associated with the numerical approximation of partial differential equations (PDEs) and ordinary differential equations (ODEs) are built up and solved.'], ['Two-grid optimality for Galerkin linear systems based on B-splines', 'A multigrid method for linear systems stemming from the Galerkin B-spline discretization of classical second-order elliptic problems is considered. The spectral features of the involved stiffness matrices, as the fineness parameter h tends to zero, have been deeply studied in previous works, with particular attention to the dependencies of the spectrum on the degree p of the B-splines used in the discretization process. Here, by exploiting this information in connection with $$\\tau $$ -matrices, we describe a multigrid strategy and we prove that the corresponding two-grid iterations have a convergence rate independent of h for $$p=1,2,3$$p=1,2,3. For larger p, the proof may be obtained through algebraic manipulations. Unfortunately, as confirmed by the numerical experiments, the dependence on p is bad and hence other techniques have to be considered for large p.'], ['Spectral behavior of preconditioned non-Hermitian multilevel block Toeplitz matrices with matrix-valued symbol', 'This note is devoted to preconditioning strategies for non-Hermitian multilevel block Toeplitz linear systems associated with a multivariate Lebesgue integrable matrix-valued symbol. In particular, we consider special preconditioned matrices, where the preconditioner has a band multilevel block Toeplitz structure, and we complement known results on the localization of the spectrum with global distribution results for the eigenvalues of the preconditioned matrices. In this respect, our main result is as follows. Let I k ( - π , π ) k , let M s be the linear space of complex s í s matrices, and let f , g : I k M s be functions whose components f ij , g ij : I k C , i , j = 1 , , s , belong to L ∞ . Consider the matrices T n - 1 ( g ) T n ( f ) , where n ( n 1 , , n k ) varies in N k and T n ( f ) , T n ( g ) are the multilevel block Toeplitz matrices of size n 1 n k s generated by f , g . Then { T n - 1 ( g ) T n ( f ) } n N k ~ λ g - 1 f , i.e. the family of matrices { T n - 1 ( g ) T n ( f ) } n N k has a global (asymptotic) spectral distribution described by the function g - 1 f , provided g possesses certain properties (which ensure in particular the invertibility of T n ( g ) for all n) and the following topological conditions are met: the essential range of g - 1 f , defined as the union of the essential ranges of the eigenvalue functions λ j ( g - 1 f ) , j = 1 , , s , does not disconnect the complex plane and has empty interior. This result generalizes the one obtained by Donatelli, Neytcheva, Serra-Capizzano in a previous work, concerning the non-preconditioned case g = 1 . The last part of this note is devoted to numerical experiments, which confirm the theoretical analysis and suggest the choice of optimal GMRES preconditioning techniques to be used for the considered linear systems.'], ['Symbol approach in a signal-restoration problem involving block Toeplitz matrices', 'We consider a special type of signal restoration problem where some of the sampling data are not available. The formulation related to samples of the function and its derivative leads to a possibly large linear system associated to a nonsymmetric block Toeplitz matrix which can be equipped with a 2x2 matrix-valued symbol. The aim of the paper is to study the eigenvalues of the matrix. We first identify in detail the symbol and its analytical features. Then, by using recent results on the eigenvalue distribution of block Toeplitz matrix-sequences, we formally describe the cluster sets and the asymptotic spectral distribution of the matrix-sequences related to our problem. The localization areas, the extremal behavior, and the conditioning are only observed numerically, but their behavior is strongly related to the analytical properties of the symbol, even though a rigorous proof is still missing in the block case.'], ['A note on the eigenvalues of $$g$$g-circulants (and of $$g$$g-Toeplitz, $$g$$g-Hankel matrices)', 'A matrix  $$A$$ A  of size  $$n$$ n  is called  $$g$$ g -circulant if  $$A=[a_{(r-g s)\\text { mod } n}]_{r,s=0}^{n-1}$$ A = [ a ( r - g s ) mod n ] r , s = 0 n - 1  , while a matrix  $$A$$ A  is called  $$g$$ g -Toeplitz if its entries obey the rule  $$A=[a_{r-g s}]_{r,s=0}^{n-1}$$ A = [ a r - g s ] r , s = 0 n - 1  . In this note we study the eigenvalues of  $$g$$ g -circulants and we provide a preliminary asymptotic analysis of the eigenvalue distribution of  $$g$$ g -Toeplitz sequences, in the case where the numbers  $$\\{a_k\\}$$ { a k }  are the Fourier coefficients of an integrable function  $$f$$ f  over the domain  $$(-\\pi ,\\pi )$$ ( - , ¿ ) : while the singular value distribution of  $$g$$ g -Toeplitz sequences is nontrivial for  $$g>1$$ g > 1 , as proved recently, the eigenvalue distribution seems to be clustered at zero and this completely different behaviour is explained by the high nonnormal character of  $$g$$ g -Toeplitz sequences when the size is large,  $$g>1$$ g > 1 , and  $$f$$ f  is not identically zero. On the other hand, for negative  $$g$$ g  the clustering at zero is proven for essentially bounded  $$f$$ f . Some numerical evidences are given and critically discussed, in connection with a conjecture concerning the zero eigenvalue distribution of  $$g$$ g -Toeplitz sequences with  $$g>1$$ g > 1  and Wiener symbol.'], ['On the spectrum of stiffness matrices arising from isogeometric analysis', 'We study the spectral properties of stiffness matrices that arise in the context of isogeometric analysis for the numerical solution of classical second order elliptic problems. Motivated by the applicative interest in the fast solution of the related linear systems, we are looking for a spectral characterization of the involved matrices. In particular, we investigate non-singularity, conditioning (extremal behavior), spectral distribution in the Weyl sense, as well as clustering of the eigenvalues to a certain (compact) subset of  $$\\mathbb C$$ C . All the analysis is related to the notion of symbol in the Toeplitz setting and is carried out both for the cases of 1D and 2D problems.'], ['Two-Grid Methods for Hermitian positive definite linear systems connected with an order relation', 'Given a multigrid procedure for linear systems with coefficient matrices  $$A_n,$$  we discuss the optimality of a related multigrid procedure with the same smoother and the same projector, when applied to properly related algebraic problems with coefficient matrices  $$B_n$$ : we assume that both  $$A_n$$  and  $$B_n$$  are Hermitian positive definite with  $$A_n\\le \\vartheta B_n,$$  for some positive  $$\\vartheta $$  independent of  $$n.$$  In this context we prove the Two-Grid Method optimality. We apply this elementary strategy for designing a multigrid solution for modifications of multilevel structured linear systems, in which the Hermitian positive definite coefficient matrix is banded in a multilevel sense. As structured matrices, Toeplitz, circulants, Hartley, sine (  $$\\tau $$  class) and cosine algebras are considered. In such a way, several linear systems arising from the approximation of integro---differential equations with various boundary conditions can be efficiently solved in linear time (with respect to the size of the algebraic problem). Some numerical experiments are presented and discussed, both with respect to Two-Grid and multigrid procedures.'], ['AMG preconditioning for nonlinear degenerate parabolic equations on nonuniform grids with application to monument degradation', 'Motivated by the modelling of marble degradation by chemical pollutants, we consider the approximation by implicit finite differences schemes of nonlinear degenerate parabolic equations in which sharp boundary layers naturally occur. The latter suggests to consider various types of nonuniform griddings, when defining suitable approximation schemes. The resulting large nonlinear systems are treated by Newton methods, while the locally Toeplitz linear systems arising from the Jacobian have to be solved efficiently. To this end, we propose the use of AMG preconditioners and we study the related convergence issues, together with the associated spectral features. We present some numerical experiments supporting our theoretical results on the spectrum of the coefficient matrix of the linear systems, alongside others regarding the numerical simulations in the case of the specific model.'], ['Multigrid methods for Toeplitz linear systems with different size reduction', 'AbstractStarting from the spectral analysis of g-circulant matrices, we study the convergence of a multigrid method for circulant and Toeplitz matrices with various size reductions. We assume that the size n of the coefficient matrix is divisible by g≥2 such that at the lower level the system is reduced to one of size n/g, by employing g-circulant based projectors. We perform a rigorous two-grid convergence analysis in the circulant case and we extend experimentally the results to the Toeplitz setting, by employing structure preserving projectors. The optimality of the two-grid method and of the multigrid method is proved, when the number θ∈ℕ of recursive calls is such that 1<θ<g. The previous analysis is used also to overcome some pathological cases, in which the generating function has zeros located at “mirror points” and the standard two-grid method with g=2 is not optimal. The numerical experiments show the correctness and applicability of the proposed ideas, both for circulant and Toeplitz matrices.'], ['Nonnegative inverse eigenvalue problems with partial eigendata', 'In this paper we consider the inverse problem of constructing an n\xa0× n real nonnegative matrix A from the prescribed partial eigendata. We first give the solvability conditions for the inverse problem without the nonnegative constraint and then discuss the associated best approximation problem. To find a nonnegative solution, we reformulate the inverse problem as a monotone complementarity problem and propose a nonsmooth Newton-type method for solving its equivalent nonsmooth equation. Under some mild assumptions, the global and quadratic convergence of our method is established. We also apply our method to the symmetric nonnegative inverse problem and to the cases of prescribed lower bounds and of prescribed entries. Numerical tests demonstrate the efficiency of the proposed method and support our theoretical findings.'], ['A note on the (regularizing) preconditioning of g -Toeplitz sequences via g -circulants', 'For a given nonnegative integer g , a matrix A n of size n is called g -Toeplitz if its entries obey the rule A n = a r - g s ] r , s = 0 n - 1 . Analogously, a matrix A n again of size n is called g -circulant if A n = a ( r - g s ) mod n ] r , s = 0 n - 1 . In a recent work we studied the asymptotic properties, in terms of spectral distribution, of both g -circulant and g -Toeplitz sequences in the case where { a k } can be interpreted as the sequence of Fourier coefficients of an integrable function f over the domain ( - π , π ) . Here we are interested in the preconditioning problem which is well understood and widely studied in the last three decades in the classical Toeplitz case, i.e., for g = 1 . In particular, we consider the generalized case with g ¿ 2 and the nontrivial result is that the preconditioned sequence { P n } = { P n - 1 A n } , where { P n } is the sequence of preconditioner, cannot be clustered at 1 so that the case of g = 1 is exceptional. However, while a standard preconditioning cannot be achieved, the result has a potential positive implication since there exist choices of g -circulant sequences which can be used as basic preconditioning sequences for the corresponding g -Toeplitz structures. Generalizations to the block and multilevel case are also considered, where g is a vector with nonnegative integer entries. A few numerical experiments, related to a specific application in signal restoration, are presented and critically discussed.'], ['Analysis of Multigrid Preconditioning for Implicit PDE Solvers for Degenerate Parabolic Equations', 'In this paper an implicit numerical method designed for nonlinear degenerate parabolic equations is proposed. A convergence analysis and the study of the related computational cost are provided. In fact, due to the nonlinear nature of the underlying mathematical model, the use of a fixed point scheme is required. The chosen scheme is the Newton method and its convergence is proven under mild assumptions. Every step of the Newton method implies the solution of large, locally structured, linear systems. A special effort is devoted to the spectral analysis of the relevant matrices and to the design of appropriate multigrid preconditioned Krylov methods. Numerical experiments for the validation of our analysis complement this contribution.'], ['Preface', 'No abstract available.'], ['Google PageRanking problem: The model and the analysis', 'The spectral and Jordan structures of the Web hyperlink matrix G(c)=cG+(1-c)ev^T have been analyzed when G is the basic (stochastic) Google matrix, c is a real parameter such that 0 ""1y(c)=Nv for all v; this limit may fail to exist for some v if @l is not semisimple. As a special case of our results, we obtain a complex analog of PageRank for the Web hyperlink matrix G(c) with a complex parameter c. We study regularity, limits, expansions, and conditioning of y(c) and we propose algorithms (e.g., complex extrapolation, power method on a modified matrix etc.) that may provide an efficient way to compute PageRank also with c close or equal to 1. An interpretation of the limit vector Nv and a related critical discussion on the model, on its adherence to reality, and possible ways for its improvement, represent the contribution of the paper on modeling issues. '], ['A note on algebraic multigrid methods for the discrete weighted Laplacian', 'In recent contributions, algebraic multigrid methods have been designed and studied from the viewpoint of spectral complementarity. In this note, we focus our efforts on specific applications and, more precisely, on large linear systems arising from the approximation of the weighted Laplacian with various boundary conditions. We adapt the multigrid idea to this specific setting and we present and critically discuss a wide set of numerical experiments showing the potentiality of the considered approach.'], ['Spectral Features and Asymptotic Properties for $g$-Circulants and $g$-Toeplitz Sequences', 'For a given nonnegative integer $g$, a matrix $A_n$ of size $n$ is called $g$-Toeplitz if its entries obey the rule $A_n=[a_{r-gs}]_{r,s=0}^{n-1}$. Analogously, a matrix $A_n$ again of size $n$ is called $g$-circulant if $A_n=\\bigl[a_{(r-g s) \\mathrm{mod}\\,n}\\bigr]_{r,s=0}^{n-1}$. Such matrices arise in wavelet analysis, subdivision algorithms, and more generally when dealing with multigrid/multilevel methods for structured matrices and approximations of boundary value problems. In this paper we study the singular values of $g$-circulants and provide an asymptotic analysis of the distribution results for the singular values of $g$-Toeplitz sequences in the case where $\\{a_k\\}$ can be interpreted as the sequence of Fourier coefficients of an integrable function $f$ over the domain $(-\\pi,\\pi)$. Generalizations to the block and multilevel case are also considered.'], ['Antireflective boundary conditions for deblurring problems', 'This survey paper deals with the use of antireflective boundary conditions for deblurring problems where the issues that we consider are the precision of the reconstruction when the noise is not present, the linear algebra related to these boundary conditions, the iterative and noniterative regularization solvers when the noise is considered, both from the viewpoint of the computational cost and from the viewpoint of the quality of the reconstruction. In the latter case, we consider a reblurring approach that replaces the transposition operation with correlation. For many of the considered items, the anti-reflective algebra coming from the given boundary conditions is the optimal choice. Numerical experiments corroborating the previous statement and a conclusion section end the paper.'], ['Stability of the notion of approximating class of sequences and applications', 'Given an approximating class of sequences {{B""n"",""m}""n}""m for {A""n}""n, we prove that {{B""n"",""m^+}""n}""m (X^+ being the pseudo-inverse of Moore-Penrose) is an approximating class of sequences for {A""n^+}""n, where {A""n}""n is a sparsely vanishing sequence of matrices A""n of size d""n with d""k>d""q for k>q,k,[email\xa0protected]?N. As a consequence, we extend distributional spectral results on the algebra generated by Toeplitz sequences, by including the (pseudo) inversion operation, in the case where the sequences that are (pseudo) inverted are distributed as sparsely vanishing symbols. Applications to preconditioning and a potential use in image/signal restoration problems are presented.'], ['On the Asymptotic Spectrum of Finite Element Matrix Sequences', 'We derive a new formula for the asymptotic eigenvalue distribution of stiffness matrices obtained by applying $P_1$ finite elements with standard mesh refinement to the semielliptic PDE of second order in divergence form $- \\nabla (K \\nabla^T u ) = f$ on Ω, $u = g$ on $\\partial \\Omega$. Here $\\Omega \\subset \\R^2$, and $K$ is supposed to be piecewise continuous and pointwise symmetric semipositive definite. The symbol describing this asymptotic eigenvalue distribution depends on the PDE, but also both on the numerical scheme for approaching the underlying bilinear form and on the geometry of triangulation of the domain. Our work is motivated by recent results on the superlinear convergence behavior of the conjugate gradient method, which requires the knowledge of such asymptotic eigenvalue distributions for sequences of matrices depending on a discretization parameter $h$ when $h \\to 0$. We compare our findings with similar results for the finite difference method which were published in recent years. In particular we observe that our sequence of stiffness matrices is part of the class of generalized locally Toeplitz sequences for which many theoretical tools are available. This enables us to derive some results on the conditioning and preconditioning of such stiffness matrices.'], ['On the analytical comparison of testing techniques', ""We introduce necessary and sufficient conditions for comparing the expected values of the number of failures caused by applications of software testing techniques. Our conditions are based only on the knowledge of a total or even a hierarchical order among the failure rates of the subdomains of a program's input domain. We also prove conditions for comparing the probability of causing at least one failure in three important special cases.""], ['A Note on Antireflective Boundary Conditions  and Fast Deblurring Models', 'In a recent work Ng, Chan, and Tang introduced reflecting (Neumann) boundary conditions (BCs) for blurring models and proved that the resulting choice leads to fast algorithms for both deblurring and detecting the regularization parameters in the presence of noise. The key point is that Neumann BC matrices can be simultaneously diagonalized by the (fast) cosine transform DCT III. Here we propose antireflective BCs that can be related to $\\tau$ structures, i.e., to the algebra of the  matrices that can be simultaneously diagonalized by the (fast) sine transform DST I. We show that, in the generic case, this is a more natural modeling whose features are (a) a reduced analytical error since the zero (Dirichlet) BCs lead to discontinuity at the boundaries, the reflecting (Neumann) BCs lead  to C0 continuity at the boundaries, while our proposal leads to C1 continuity at the boundaries; (b) fast numerical algorithms in real arithmetic for both deblurring and estimating regularization parameters. Finally, simple yet significant 1D and 2D numerical evidence is presented and discussed.']]"
Marco Cerami,[]
Anurat Chapanond,[]
Ajo Fod,[]
Gilles Degottex,"[['A Log Domain Pulse Model for Parametric Speech Synthesis', 'Most of the degradation in current Statistical Parametric Speech Synthesis SPSS results from the form of the vocoder. One of the main causes of degradation is the reconstruction of the noise. In this article, a new signal model is proposed that leads to a simple synthesizer, without the need for ad-hoc tuning of model parameters. The model is not based on the traditional additive linear source-filter model, it adopts a combination of speech components that are additive in the log domain. Also, the same representation for voiced and unvoiced segments is used, rather than relying on binary voicing decisions. This avoids voicing error discontinuities that can occur in many current vocoders. A simple binary mask is used to denote the presence of noise in the time-frequency domain, which is less sensitive to classification errors. Four experiments have been carried out to evaluate this new model. The first experiment examines the noise reconstruction issue. Three listening tests have also been carried out that demonstrate the advantages of this model: comparison with the STRAIGHT vocoder; the direct prediction of the binary noise mask by using a mixed output configuration; and partial improvements of creakiness using a mask correction mechanism.'], ['Multi-frame amplitude envelope estimation for modification of singing voice', 'Singingvoice synthesis benefits from very highquality estimation of the resonances and anti-resonances of the vocal tract filter (VTF), i.e., an amplitude spectral envelope. In the state of the art, a single frame of DFT transform is commonly used as a basis for building spectral envelopes. Even though multiple frame analysis (MFA) has already been suggested for envelope estimation, it is not yet used in concrete applications. Indeed, even though existing attempts have shown very interesting results, we will demonstrate that they are either over complicated or fail to satisfy the high accuracy that is necessary for singing voice. In order to allow future applications of MFA, this article aims to improve the theoretical understanding and advantages of MFA-based methods. The use of singing voice signals is very beneficial for studying MFA methods due to the fact that the VTF configuration can be relatively stable and, at the same time, the vibrato creates a regular variation that is easy to model. By simplifying and extending previous works, we also suggest and describe two MFA-based methods. To better understand the behaviors of the envelope estimates, we designed numerical measurements to assess single frame analysis and MFA methods using synthetic signals. With listening tests, we also designed two proofs of concept using pitch scaling and conversion of timbre. Both evaluations show clear and positive results for MFA-based methods, thus, encouraging this research direction for future applications.'], ['Simple multi frame analysis methods for estimation of amplitude spectral envelope estimation in singing voice', 'In the state of the art, a single frame of DFT transform is commonly used as a basis for building amplitude spectral envelopes. Multiple Frame Analysis (MFA) has already been suggested for envelope estimation, but often with excessive complexity. In this paper, two MFA-based methods are presented: one simplifying an existing Least Square (LS) solution, and another one based on a simple linear interpolation. In the context of singing voice we study sustained segments with vibrato, because these ones are obviously critical for singing voice synthesis. They also provide a convenient context to study, prior to extension of this work in more general contexts. Numerical and perceptual experiments show clear improvements of the two methods described compared to the state of the art and encourage further studies in this research direction.'], ['Speech analysis and synthesis with a computationally efficient adaptive harmonic model', 'Harmonic models have to be both precise and fast in order to represent the speech signal adequately and be able to process large amount of data in a reasonable amount of time. For these purposes, the full-band adaptive harmonic model (aHM) used by the adaptive iterative refinement (AIR) algorithm has been proposed in order to accurately model the perceived characteristics of a speech signal. Even though aHM-AIR is precise, it lacks the computational efficiency that would make its use convenient for large databases. The least squares (LS) solution used in the original aHM-AIR accounts for most of the computational load. In a previous paper, we suggested a peak picking (PP) approach as a substitution to the LS solution. In order to integrate the adaptivity scheme of aHM in the PP approach, an adaptive discrete Fourier transform (aDFT), whose frequency basis can fully follow the variations of the curve, was also proposed. In this paper, we complete the previous publication by evaluating the above methods for the whole analysis process of a speech signal. Evaluations have shown an average time reduction by four times using PP and aDFT compared to the LS solution. Additionally, based on formal listening tests, when using PP and aDFT, the quality of the re-synthesis is preserved compared to the original LS-based approach.'], ['Analysis and Synthesis of Speech Using an Adaptive Full-Band Harmonic Model', 'Voice models often use frequency limits to split the speech spectrum into two or more voiced/unvoiced frequency bands. However, from the voice production, the amplitude spectrum of the voiced source decreases smoothly without any abrupt frequency limit. Accordingly, multiband models struggle to estimate these limits and, as a consequence, artifacts can degrade the perceived quality. Using a linear frequency basis adapted to the non-stationarities of the speech signal, the Fan Chirp Transformation (FChT) have demonstrated harmonicity at frequencies higher than usually observed from the DFT which motivates a full-band modeling. The previously proposed Adaptive Quasi-Harmonic model (aQHM) offers even more flexibility than the FChT by using a non-linear frequency basis. In the current paper, exploiting the properties of aQHM, we describe a full-band Adaptive Harmonic Model (aHM) along with detailed descriptions of its corresponding algorithms for the estimation of harmonics up to the Nyquist frequency. Formal listening tests show that the speech reconstructed using aHM is nearly indistinguishable from the original speech. Experiments with synthetic signals also show that the proposed aHM globally outperforms previous sinusoidal and harmonic models in terms of precision in estimating the sinusoidal parameters. As a perspective, such a precision is interesting for building higher level models upon the sinusoidal parameters, like spectral envelopes for speech synthesis.'], ['Mixed source model and its adapted vocal tract filter estimate for voice transformation and synthesis', 'In current methods for voice transformation and speech synthesis, the vocal tract filter is usually assumed to be excited by a flat amplitude spectrum. In this article, we present a method using a mixed source model defined as a mixture of the Liljencrants-Fant (LF) model and Gaussian noise. Using the LF model, the base approach used in this presented work is therefore close to a vocoder using exogenous input like ARX-based methods or the Glottal Spectral Separation (GSS) method. Such approaches are therefore dedicated to voice processing promising an improved naturalness compared to generic signal models. To estimate the Vocal Tract Filter (VTF), using spectral division like in GSS, we show that a glottal source model can be used with any envelope estimation method conversely to ARX approach where a least square AR solution is used. We therefore derive a VTF estimate which takes into account the amplitude spectra of both deterministic and random components of the glottal source. The proposed mixed source model is controlled by a small set of intuitive and independent parameters. The relevance of this voice production model is evaluated, through listening tests, in the context of resynthesis, HMM-based speech synthesis, breathiness modification and pitch transposition.'], ['Phase Minimization for Glottal Model Estimation', 'In glottal source analysis, the phase minimization criterion has already been proposed to detect excitation instants. As shown in this paper, this criterion can also be used to estimate the shape parameter of a glottal model (ex. Liljencrants-Fant model) and not only its time position. Additionally, we show that the shape parameter can be estimated independently of the glottal model position. The reliability of the proposed methods is evaluated with synthetic signals and compared to that of the IAIF and minimum/maximum-phase decomposition methods. The results of the methods are evaluated according to the influence of the fundamental frequency and noise. The estimation of a glottal model is useful for the separation of the glottal source and the vocal-tract filter and therefore can be applied in voice transformation, synthesis, and also in clinical context or for the study of the voice production.']]"
Erik Bylow,[]
Pierre Bonami,"[['Implementing Automatic Benders Decomposition in a Modern MIP Solver', 'AbstractWe describe the automatic Benders decomposition implemented in the commercial solver IBM CPLEX. We propose several improvements to the state-of-the-art along two lines: making a numerically robust method able to deal with the general case and improving the efficiency of the method on models amenable to decomposition. For the former, we deal with: unboundedness, failures in generating cuts and scaling of the artificial variable representing the objective. For the latter, we propose a new technique to handle so-called generalized bound constraints and we use different types of normalization conditions in the Cut Generating LPs. We present computational experiments aimed at assessing the importance of the various enhancements. In particular, on our test bed of models amenable to a decomposition, our implementation is approximately 5 times faster than CPLEX default branch-and-cut. A remarkable result is that, on the same test bed, default branch-and-cut is faster than a Benders decomposition that doesn’t implement our improvements.'], ['Maximum flow under proportional delay constraint', 'Given a network and a set of source destination pairs (connections), we consider the problem of maximizing the sum of the flow under proportional delay constraints. In this paper, the delay for crossing a link is proportional to the total flow crossing this link. If a connection supports non-zero flow, then the sum of the delays along any path corresponding to that connection must be lower than a given bound. The constraints of delay are onoff constraints because if a connection carries zero flow, then there is no constraint for that connection. The difficulty of the problem comes from the choice of the connections supporting non-zero flow. We first prove a general approximation ratio using linear programming for a variant of the problem. We then prove a linear time 2-approximation algorithm when the network is a path. We finally show a Polynomial Time Approximation Scheme when the graph of intersections of the paths has bounded treewidth.'], ['On handling indicator constraints in mixed integer programming', 'Mixed integer programming (MIP) is commonly used to model indicator constraints, i.e., constraints that either hold or are relaxed depending on the value of a binary variable. Unfortunately, those models tend to lead to weak continuous relaxations and turn out to be unsolvable in practice; this is what happens, for e.g., in the case of Classification problems with Ramp Loss functions that represent an important application in this context. In this paper we show the computational evidence that a relevant class of these Classification instances can be solved far more efficiently if a nonlinear, nonconvex reformulation of the indicator constraints is used instead of the linear one. Inspired by this empirical and surprising observation, we show that aggressive bound tightening is the crucial ingredient for solving this class of instances, and we devise a pair of computationally effective algorithmic approaches that exploit it within MIP. One of these methods is currently part of the arsenal of IBM-Cplex since version 12.6.1. More generally, we argue that aggressive bound tightening is often overlooked in MIP, while it represents a significant building block for enhancing MIP technology when indicator constraints and disjunctive terms are present.'], ['Cut generation through binarization', 'For a mixed integer linear program where all integer variables are bounded, we study a reformulation introduced by Roy that maps general integer variables to a collection of binary variables. We study theoretical properties and empirical strength of rank-2 simple split cuts of the reformulation. We show that for a pure integer problem with two integer variables, these cuts are sufficient to obtain the integer hull of the problem, but that this does not generalize to problems in higher dimensions. We also give an algorithm to compute an approximation of the rank-2 simple split cut closure. We report empirical results on 22 benchmark instances showing that the bounds obtained compare favorably with those obtained with other approximate methods to compute the split closure or lattice-free cut closure. It gives a better bound than the split closure on 6 instances while it is weaker on 9 instances, for an average gap closed 3.8 % smaller than the one for the split closure.'], ['On mathematical programming with indicator constraints', 'In this paper we review the relevant literature on mathematical optimization with logical implications, i.e., where constraints can be either active or disabled depending on logical conditions to hold. In the case of convex functions, the theory of disjunctive programming allows one to formulate these logical implications as convex nonlinear programming problems in a space of variables lifted with respect to its original dimension. We concentrate on the attempt of avoiding the issue of dealing with large NLPs. In particular, we review some existing results that allow to work in the original space of variables for two relevant special cases where the disjunctions corresponding to the logical implications have two terms. Then, we significantly extend these special cases in two different directions, one involving more general convex sets and the other with disjunctions involving three terms. Computational experiments comparing disjunctive programming formulations in the original space of variables with straightforward bigM ones show that the former are computationally viable and promising.'], ['Energy-Optimal Multi-Goal Motion Planning for Planar Robot Manipulators', 'In this work, the energy-optimal motion planning problem for planar robot manipulators with two revolute joints is studied, in which the end-effector of the robot manipulator is constrained to pass through a set of waypoints, whose sequence is not predefined. This multi-goal motion planning problem has been solved as a mixed-integer optimal control problem in which, given the dynamic model of the robot manipulator, the initial and final configurations of the robot, and a set of waypoints inside the workspace of the manipulator, one has to find the control inputs, the sequence of waypoints with the corresponding passage times, and the resulting trajectory of the robot that minimizes the energy consumption during the motion. The presence of the waypoint constraints makes this optimal control problem particularly difficult to solve. The mixed-integer optimal control problem has been converted into a mixed-integer nonlinear programming problem first making the unknown passage times through the waypoints part of the state, then introducing binary variables to enforce the constraint of passing once through each waypoint, and finally applying a fifth-degree Gauss---Lobatto direct collocation method to tackle the dynamic constraints. High-degree interpolation polynomials allow the number of variables of the problem to be reduced for a given numerical precision. The resulting mixed-integer nonlinear programming problem has been solved using a nonlinear programming-based branch-and-bound algorithm specifically tailored to the problem. The results of the numerical experiments have shown the effectiveness of the approach.'], ['On branching rules for convex mixed-integer nonlinear optimization', 'Branch-and-Bound (B&B) is perhaps the most fundamental algorithm for the global solution of convex Mixed-Integer Nonlinear Programming (MINLP) problems. It is well-known that carrying out branching in a nonsimplistic manner can greatly enhance the practicality of B&B in the context of Mixed-Integer Linear Programming (MILP). No detailed study of branching has heretofore been carried out for MINLP. In this article, we study and identify useful sophisticated branching methods for MINLP, including novel approaches based on approximations of the nonlinear relaxations by linear and quadratic programs.'], ['Preface', 'No abstract available.'], ['Mixed-integer nonlinear programs featuring ""on/off"" constraints', 'In this paper, we study MINLPs featuring ""on/off"" constraints. An ""on/off"" constraint is a constraint  f (  x ) 0 that is activated whenever a corresponding 0---1 variable is equal to 1. Our main result is an explicit characterization of the convex hull of the feasible region when the MINLP consists of simple bounds on the variables and one ""on/off"" constraint defined by an isotone function  f . When extended to general convex MINLPs, we show that this result yields tight lower bounds compared to classical formulations. This allows us to introduce new models for the delay-constrained routing problem in telecommunications. Numerical results show gains in computing time of up to one order of magnitude compared to state-of-the-art approaches.'], ['Heuristics for convex mixed integer nonlinear programs', 'In this paper, we describe the implementation of some heuristics for convex mixed integer nonlinear programs. The work focuses on three families of heuristics that have been successfully used for mixed integer linear programs: diving heuristics, the Feasibility Pump, and Relaxation Induced Neighborhood Search (RINS). We show how these heuristics can be adapted in the context of mixed integer nonlinear programming. We present results from computational experiments on a set of instances that show how the heuristics implemented help finding feasible solutions faster than the traditional branch-and-bound algorithm and how they help in reducing the total solution time of the branch-and-bound algorithm.'], ['Convex relaxations of non-convex mixed integer quadratically constrained programs: projected formulations', 'A common way to produce a convex relaxation of a Mixed Integer Quadratically Constrained Program (MIQCP) is to lift the problem into a higher-dimensional space by introducing variables Y ij to represent each of the products x i x j of variables appearing in a quadratic form. One advantage of such extended relaxations is that they can be efficiently strengthened by using the (convex) SDP constraint $${Y - x x^T \\succeq 0}$$ and disjunctive programming. On the other hand, the main drawback of such an extended formulation is its huge size, even for problems for which the number of x i variables is moderate. In this paper, we study methods to build low-dimensional relaxations of MIQCP that capture the strength of the extended formulations. To do so, we use projection techniques pioneered in the context of the lift-and-project methodology. We show how the extended formulation can be algorithmically projected to the original space by solving linear programs. Furthermore, we extend the technique to project the SDP relaxation by solving SDPs. In the case of an MIQCP with a single quadratic constraint, we propose a subgradient-based heuristic to efficiently solve these SDPs. We also propose a new eigen-reformulation for MIQCP, and a cut generation technique to strengthen this reformulation using polarity. We present extensive computational results to illustrate the efficiency of the proposed techniques. Our computational results have two highlights. First, on the GLOBALLib instances, we are able to generate relaxations that are almost as strong as those proposed in our companion paper even though our computing times are about 100 times smaller, on average. Second, on box-QP instances, the strengthened relaxations generated by our code are almost as strong as the well-studied SDP+RLT relaxations and can be solved in less than 2\xa0s, even for large instances with 100 variables; the SDP+RLT relaxations for the same set of instances can take up to a couple of hours to solve using a state-of-the-art SDP solver.'], ['Experiments with Two-Row Cuts from Degenerate Tableaux', 'There has been a recent interest in cutting planes generated from two or more rows of the optimal simplex tableau. One can construct examples of integer programs for which a single cutting plane generated from two rows dominates the entire split closure. Motivated by these theoretical results, we study the effect of adding a family of cutting planes generated from two rows on a set of instances from the MIPLIB library. The conclusion of whether these cuts are competitive with Gomory mixed-integer cuts is very sensitive to the experimental setup. In particular, we consider the issue of reliability versus aggressiveness of the cut generators, an issue that is usually not addressed in the literature.'], ['Lift-and-project cuts for mixed integer convex programs', 'This paper addresses the problem of generating cuts for mixed integer nonlinear programs where the objective is linear and the relations between the decision variables are described by convex functions defining a convex feasible region. We propose a new method for strengthening the continuous relaxations of such problems using cutting planes. Our method can be seen as a practical implementation of the lift-and-project technique in the nonlinear case. To derive each cut we use a combination of a nonlinear programming subproblem and a linear outer approximation. One of the main features of the approach is that the subproblems solved to generate cuts are typically not more complicated than the original continuous relaxation. In particular they do not require the introduction of additional variables or nonlinearities. We propose several strategies for using the technique and present preliminary computational evidence of its practical interest. In particular, the cuts allow us to improve over the state of the art branch-and-bound of the solver Bonmin, solving more problems in faster computing times on average.'], ['On the relative strength of split, triangle and quadrilateral cuts', 'Integer programs defined by two equations with two free integer variables and nonnegative continuous variables have three types of nontrivial facets: split, triangle or quadrilateral inequalities. In this paper, we compare the strength of these three families of inequalities. In particular we study how well each family approximates the integer hull. We show that, in a well defined sense, triangle inequalities provide a good approximation of the integer hull. The same statement holds for quadrilateral inequalities. On the other hand, the approximation produced by split inequalities may be arbitrarily bad.'], ['Convex relaxations of non-convex mixed integer quadratically constrained programs: extended formulations', 'This paper addresses the problem of generating strong convex relaxations of Mixed Integer Quadratically Constrained Programming (MIQCP) problems. MIQCP problems are very difficult because they combine two kinds of non- convexities: integer variables and non-convex quadratic constraints. To produce strong relaxations of MIQCP problems, we use techniques from disjunctive programming and the lift-and-project methodology. In particular, we propose new methods for generating valid inequalities from the equation Y = xxT. We use the non-convex constraint $${ Y - x x^T \\preccurlyeq 0}$$ to derive disjunctions of two types. The first ones are directly derived from the eigenvectors of the matrix Y ź xxT with positive eigenvalues, the second type of disjunctions are obtained by combining several eigenvectors in order to minimize the width of the disjunction. We also use the convex SDP constraint $${ Y - x x^T \\succcurlyeq 0}$$ to derive convex quadratic cuts, and we combine both approaches in a cutting plane algorithm. We present computational results to illustrate our findings.'], ['Convex relaxations of non-convex mixed integer quadratically constrained programs: extended formulations', 'This paper addresses the problem of generating strong convex relaxations of Mixed Integer Quadratically Constrained Programming (MIQCP) problems. MIQCP problems are very difficult because they combine two kinds of non- convexities: integer variables and non-convex quadratic constraints. To produce strong relaxations of MIQCP problems, we use techniques from disjunctive programming and the lift-and-project methodology. In particular, we propose new methods for generating valid inequalities from the equation Y =\xa0 x x T. We use the non-convex constraint $${ Y - x x^T \\preccurlyeq 0}$$ to derive disjunctions of two types. The first ones are directly derived from the eigenvectors of the matrix Y − x x T with positive eigenvalues, the second type of disjunctions are obtained by combining several eigenvectors in order to minimize the width of the disjunction. We also use the convex SDP constraint $${ Y - x x^T \\succcurlyeq 0}$$to derive convex quadratic cuts, and we combine both approaches in a cutting plane algorithm. We present computational results to illustrate our findings.'], ['An Exact Solution Approach for Portfolio Optimization Problems Under Stochastic and Integer Constraints', 'In this paper, we study extensions of the classical Markowitz mean-variance portfolio optimization model. First, we consider that the expected asset returns are stochastic by introducing a probabilistic constraint, which imposes that the expected return of the constructed portfolio must exceed a prescribed return threshold with a high confidence level. We study the deterministic equivalents of these models. In particular, we define under which types of probability distributions the deterministic equivalents are second-order cone programs and give closed-form formulations. Second, we account for real-world trading constraints (such as the need to diversify the investments in a number of industrial sectors, the nonprofitability of holding small positions, and the constraint of buying stocks by lots) modeled with integer variables. To solve the resulting problems, we propose an exact solution approach in which the uncertainty in the estimate of the expected returns and the integer trading restrictions are simultaneously considered. The proposed algorithmic approach rests on a nonlinear branch-and-bound algorithm that features two new branching rules. The first one is a static rule, called idiosyncratic risk branching, while the second one is dynamic and is called portfolio risk branching. The two branching rules are implemented and tested using the open-source <monospace>Bonmin</monospace> framework. The comparison of the computational results obtained with state-of-the-art MINLP solvers (<monospace>MINLP_BB</monospace> and <monospace>CPLEX</monospace>) and with our approach shows the effectiveness of the latter, which permits to solve to optimality problems with up to 200 assets in a reasonable amount of time. The practicality of the approach is illustrated through its use for the construction of four fund-of-funds now available on the major trading markets.'], ['A Feasibility Pump for mixed integer nonlinear programs', 'We present an algorithm for finding a feasible solution to a convex mixed integer nonlinear program. This algorithm, called Feasibility Pump, alternates between solving nonlinear programs and mixed integer linear programs. We also discuss how the algorithm can be iterated so as to improve the first solution it finds, as well as its integration within an outer approximation scheme. We report computational results.'], ['On the relative strength of split, triangle and quadrilateral cuts', 'Integer programs defined by two equations with two free integer variables and nonnegative continuous variables have three types of nontrivial facets: split, triangle or quadrilateral inequalities. In this paper, we compare the strength of these three families of inequalities. In particular we study how well each family approximates the integer hull. We show that, in a well defined sense, triangle inequalities provide a good approximation of the integer hull. The same statement holds for quadrilateral inequalities. On the other hand, the approximation produced by split inequalities may be arbitrarily bad.'], ['Projected Chvátal---Gomory cuts for mixed integer linear programs', 'Recent experiments by Fischetti and Lodi show that the first Chvátal closure of a pure integer linear program (ILP) often gives a surprisingly tight approximation of the integer hull. They optimize over the first  Chvátal closure by modeling the Chvátal---Gomory (CG) separation problem as a mixed integer linear program (MILP) which is then solved by a general- purpose MILP solver. Unfortunately, this approach does not extend immediately to the Gomory mixed integer (GMI) closure of an MILP, since the GMI separation problem involves the solution of a nonlinear mixed integer program or a parametric MILP. In this paper we introduce a projected version of the CG cuts, and study their practical effectiveness for MILP problems. The idea is to project first the linear programming relaxation of the MILP at hand onto the space of the integer variables, and then to derive Chvátal---Gomory cuts for the projected polyhedron. Though theoretically dominated by GMI cuts, projected CG cuts have the advantage of producing a separation model very similar to the one introduced by Fischetti and Lodi, which can typically be solved in a reasonable amount of computing time.'], ['Disjunctive cuts for non-convex mixed integer quadratically constrained programs', 'This paper addresses the problem of generating strong convex relaxations of Mixed Integer Quadratically Constrained Programming (MIQCP) problems. MIQCP problems are very difficult because they combine two kinds of non-convexities: integer variables and nonconvex quadratic constraints. To produce strong relaxations of MIQCP problems, we use techniques from disjunctive programming and the lift-and-project methodology. In particular, we propose new methods for generating valid inequalities by using the equation Y = xxT. We use the concave constraint 0 ≥ Y - xxT to derive disjunctions of two types. The first ones are directly derived from the eigenvectors of the matrix Y - xxT with positive eigenvalues, the second type of disjunctions are obtained by combining several eigenvectors in order to minimize the width of the disjunction. We also use the convex SDP constraint Y - xxT ≥ 0 to derive convex quadratic cuts and combine both approaches in a cutting plane algorithm. We present preliminary computational results to illustrate our findings.'], ['An algorithmic framework for convex mixed integer nonlinear programs', 'This paper is motivated by the fact that mixed integer nonlinear programming is an important and difficult area for which there is a need for developing new methods and software for solving large-scale problems. Moreover, both fundamental building blocks, namely mixed integer linear programming and nonlinear programming, have seen considerable and steady progress in recent years. Wishing to exploit expertise in these areas as well as on previous work in mixed integer nonlinear programming, this work represents the first step in an ongoing and ambitious project within an open-source environment. COIN-OR is our chosen environment for the development of the optimization software. A class of hybrid algorithms, of which branch-and-bound and polyhedral outer approximation are the two extreme cases, are proposed and implemented. Computational results that demonstrate the effectiveness of this framework are reported. Both the library of mixed integer nonlinear problems that exhibit convex continuous relaxations, on which the experiments are carried out, and a version of the software used are publicly available.'], ['Projected Chvátal–Gomory cuts for mixed integer linear programs', 'Recent experiments by Fischetti and Lodi show that the first Chvátal closure of a pure integer linear program (ILP) often gives a surprisingly tight approximation of the integer hull. They optimize over the first Chvátal closure by modeling the Chvátal–Gomory (CG) separation problem as a mixed integer linear program (MILP) which is then solved by a general- purpose MILP solver. Unfortunately, this approach does not extend immediately to the Gomory mixed integer (GMI) closure of an MILP, since the GMI separation problem involves the solution of a nonlinear mixed integer program or a parametric MILP. In this paper we introduce a projected version of the CG cuts, and study their practical effectiveness for MILP problems. The idea is to project first the linear programming relaxation of the MILP at hand onto the space of the integer variables, and then to derive Chvátal–Gomory cuts for the projected polyhedron. Though theoretically dominated by GMI cuts, projected CG cuts have the advantage of producing a separation model very similar to the one introduced by Fischetti and Lodi, which can typically be solved in a reasonable amount of computing time.'], ['A note on the MIR closure', 'In 1988, Nemhauser and Wolsey introduced the concept of MIR inequality for mixed integer linear programs. In 1998, Wolsey gave another definition of MIR inequalities. This note points out that the natural concepts of MIR closures derived from these two definitions are distinct. Dash, Gunluk and Lodi made the same observation independently.'], ['New Variants of Lift-and-Project Cut Generation from the LP Tableau: Open Source Implementation and Testing', 'We discuss an open source implementation and preliminary computational testing of three variants of the Balas-Perregaard procedure for generating lift-and-project cuts from the original simplex tableau, two of which are new. Variant 1 is the original procedure of [6] with minor modifications. Variant 2 uses a new procedure for choosing the pivot element: After identifying the set of row candidates for an improving pivot, the pivot element (and column) is chosen by optimizing over the entries of all candidate rows. Finally, Variant 3 replaces the source row with its disjunctive modularization, and after each pivot it again modularizes the resulting source row. We report on computational results with the above three variants and their combinations on 65 MIPLIB.3 instances.'], ['Exact MAX-2SAT solution via lift-and-project closure', 'We present a new approach for exact solution of MAX-2SAT problems based on a strong reformulation deduced from an optimal continuous solution over the elementary closure of lift-and-project cuts. Computational results show that this formulation leads to a reduced number of nodes in the branch-and-bound tree and short computing times.'], ['Using rank-1 lift-and-project closures to generate cuts for 0-1 MIPs, a computational investigation', 'Various techniques for building relaxations and generating valid inequalities for pure or mixed integer programming problems without special structure are reviewed and compared computationally. Besides classical techniques such as Gomory cuts, Mixed Integer Rounding cuts, lift-and-project and reformulation-linearization techniques, a new variant is also investigated: the use of the relaxation corresponding to the intersection of simple disjunction polyhedra (i.e. the so-called elementary closure of lift-and-project cuts). Systematic comparative computational results are reported on series of test problems including multidimensional knapsack problems (MKP) and MIPLIB test problems. From the results obtained, the relaxation based on the elementary closure of lift-and-project cuts appears to be one of the most promising.']]"
Daniel Stutzbach,"[['Montra: A large-scale DHT traffic monitor', 'This paper presents a new technique, called Montra, for accurately capturing traffic in a widely deployed DHT. The basic idea is to make the traffic monitors minimally visible to participating peers to avoid disruption in the system. We describe how Montra leverages the required redundancy in published content and routing to minimize disruption of the system. Validations of Montra over two widely deployed DHTs, namely Kad and Azureus, show that it can accurately capture more than 90% of traffic destined to monitored peers. Furthermore, the lightweight nature of Montra allows it to monitor a large number of peers with a moderate amount of resources. We use Montra to characterize several aspects of traffic in our two target DHTs.'], ['Large-scale monitoring of DHT traffic', 'Studying deployed Distributed Hash Tables (DHTs) entails monitoring DHT traffic. Commonly, DHT traffic is measured by instrumenting ordinary peers to passively record traffic. In this approach, using a small number of peers leads to a limited (and potentially biased) view of traffic. Alternatively, inserting a large number of peers may disrupt the natural traffic patterns of the DHT and lead to incorrect results. In general, accurately capturing DHT traffic is a challenging task. In this paper, we propose the idea of minimally visible monitors to capture the traffic at a large number of peers with minimum disruption to the DHT. We implement and validate our proposed technique, called Montra, on the Kad DHT. We show that Montra accurately captures around 90% of the query traffic while monitoring roughly 32,000 peers and can accurately identify destination peers for 90% of captured destination traffic. Using Montra, we characterize the traffic in Kad and present our preliminary results.'], ['On unbiased sampling for unstructured peer-to-peer networks', 'This paper presents a detailed examination of how the dynamic and heterogeneous nature of real-world peer-to-peer systems can introduce bias into the selection of representative samples of peer properties (e.g., degree, link bandwidth, number of files shared). We propose the Metropolized Random Walk with Backtracking (MRWB) as a viable and promising technique for collecting nearly unbiased samples and conduct an extensive simulation study to demonstrate that our technique works well for a wide variety of commonly-encountered peer-to-peer network conditions. We have implemented the MRWB algorithm for selecting peer addresses uniformly at random into a tool called ion-sampler. Using the Gnutella network, we empirically show that ion-sampler. yields more accurate samples than tools that rely on commonly-used sampling techniques and results in dramatic improvements in efficiency and scalability compared to performing a full crawl.'], ['Characterizing unstructured overlay topologies in modern P2P file-sharing systems', 'In recent years, peer-to-peer (P2P) file-sharing systems have evolved to accommodate growing numbers of participating peers. In particular, new features have changed the properties of the unstructured overlay topologies formed by these peers. Little is known about the characteristics of these topologies and their dynamics in modern file-sharing applications, despite their importance. This paper presents a detailed characterization of P2P overlay topologies and their dynamics, focusing on the modern Gnutella network. We present Cruiser, a fast and accurate P2P crawler, which can capture a complete snapshot of the Gnutella network of more than one million peers in just a few minutes, and show how inaccuracy in snapshots can lead to erroneous conclusions--such as a power-law degree distribution. Leveraging recent overlay snapshots captured with Cruiser, we characterize the graph-related properties of individual overlay snapshots and overlay dynamics across slices of back-to-back snapshots. Our results reveal that while the Gnutella network has dramatically grown and changed in many ways, it still exhibits the clustering and short path lengths of a small world network. Furthermore, its overlay topology is highly resilient to random peer departure and even systematic attacks. More interestingly, overlay dynamics lead to an ""onion-like"" biased connectivity among peers where each peer is more likely connected to peers with higher uptime. Therefore, long-lived peers form a stable core that ensures reachability among peers despite overlay dynamics.'], ['Characterizing files in the modern Gnutella network', 'The Internet has witnessed an explosive increase in the popularity of Peer-to-Peer (P2P) file-sharing applications during the past few years. As these applications become more popular, it becomes increasingly important to characterize their behavior in order to improve their performance and quantify their impact on the network. In this paper, we present a measurement study on characteristics of available files in the modern Gnutella system. We develop two new methodologies to capture accurate snapshots of available files in a large-scale P2P system. These methodologies were implemented in a parallel crawler that captures the entire overlay topology of the system where each peer in the overlay is annotated with its available files. We have captured more than 50 snapshots of the Gnutella system that span over 1 year period. Using these snapshots, we conduct three types of analysis on available files: (1) Static analysis, (2) Topological analysis, and (3) dynamic analysis. Our results reveal several interesting properties of available files in Gnutella that can be leveraged to improve the design and evaluation of P2P file-sharing applications.'], ['Understanding churn in peer-to-peer networks', 'The dynamics of peer participation, or churn, are an inherent property of Peer-to-Peer (P2P) systems and critical for design and evaluation. Accurately characterizing churn requires precise and unbiased information about the arrival and departure of peers, which is challenging to acquire. Prior studies show that peer participation is highly dynamic but with conflicting characteristics. Therefore, churn remains poorly understood, despite its significance.In this paper, we identify several common pitfalls that lead to measurement error. We carefully address these difficulties and present a detailed study using three widely-deployed P2P systems: an unstructured file-sharing system (Gnutella), a content-distribution system (BitTorrent), and a Distributed Hash Table (Kad). Our analysis reveals several properties of churn: (i) overall dynamics are surprisingly similar across different systems, (ii) session lengths are not exponential, (iii) a large portion of active peers are highly stable while the remaining peers turn over quickly, and (iv) peer session lengths across consecutive appearances are correlated. In summary, this paper advances our understanding of churn by improving accuracy, comparing different P2P file sharingdistribution systems, and exploring new aspects of churn.'], ['On unbiased sampling for unstructured peer-to-peer networks', 'This paper addresses the difficult problem of selecting representative samples of peer properties (eg degree, link bandwidth, number of files shared) in unstructured peer-to-peer systems. Due to the large size and dynamic nature of these systems, measuring the quantities of interest on every peer is often prohibitively expensive, while sampling provides a natural means for estimating system-wide behavior efficiently. However, commonly-used sampling techniques for measuring peer-to-peer systems tend to introduce considerable bias for two reasons. First, the dynamic nature of peers can bias results towards short-lived peers, much as naively sampling flows in a router can lead to bias towards short-lived flows. Second, the heterogeneous nature of the overlay topology can lead to bias towards high-degree peers.We present a detailed examination of the ways that the behavior of peer-to-peer systems can introduce bias and suggest the  Metropolized Random Walk with Backtracking (MRWB) as a viable and promising technique for collecting nearly unbiased samples. We conduct an extensive simulation study to demonstrate that the proposed technique works well for a wide variety of common peer-to-peer network conditions. Using the Gnutella network, we empirically show that our implementation of the MRWB technique yields more accurate samples than relying on commonly-used sampling techniques. Furthermore, we provide insights into the causes of the observed differences. The tool we have developed, ion-sampler, selects peer addresses uniformly at random using the MRWB technique. These addresses may then be used as input to another measurement tool to collect data on a particular property.'], ['Characterizing unstructured overlay topologies in modern P2P file-sharing systems', 'During recent years, peer-to-peer (P2P) file-sharing systems have evolved in many ways to accommodate growing numbers of participating peers. In particular, new features have changed the properties of the unstructured overlay topology formed by these peers. Despite their importance, little is known about the characteristics of these topologies and their dynamics in modern file-sharing applications. This paper presents a detailed characterization of P2P overlay topologies and their dynamics, focusing on the modern Gnutella network. Using our fast and accurate P2P crawler, we capture a complete snapshot of the Gnutella network with more than one million peers in just a few minutes. Leveraging more than 18,000 recent overlay snapshots, we characterize the graph-related properties of individual overlay snapshots and overlay dynamics across hundreds of back-to-back snapshots. We show how inaccuracy in snapshots can lead to erroneous conclusions--such as a power-law degree distribution. Our results reveal that while the Gnutella network has dramatically grown and changed in many ways, it still exhibits the clustering and short path lengths of a small world network. Furthermore, its overlay topology is highly resilient to random peer departure and even systematic attacks. More interestingly, overlay dynamics lead to an ""onion-like"" biased connectivity among peers where each peer is more likely connected to peers with higher uptime. Therefore, long-lived peers form a stable core that ensures reachability among peers despite overlay dynamics.'], ['Characterizing the two-tier gnutella topology', 'Characterizing the properties of peer-to-peer (P2P) overlay topologies in file-sharing applications is essential for understanding their impact on the network, identifying their performance bottlenecks in practice, and evaluating their performance via simulation. Such characterization requires accurate snapshots of the overlay topology which is difficult to capture due to the large size and dynamic nature. Previous studies characterizing overlay topologies not only are outdated but also rely on partial or potentially distorted snapshots. In this extended abstract, we briefly present the first characterization of two-tier Gnutella topologies based on recent and accurate snapshots.'], ['The scalability of swarming peer-to-peer content delivery', 'Most web sites are unable to serve content to a large number of users due to the inherent limitations of client-server file transfer. Recent peer-to-peer content delivery protocols have demonstrated the feasibility of spreading this load among the clients themselves, giving small web sites the possibility of serving large audiences with very low cost. In this paper we use a simulation-based performance evaluation to study the fundamental question of the scalability of swarming peer-topeer content delivery. Our results demonstrate the superior scalability of swarming with respect to load, file size, block size, and client bandwidth.'], ['Evaluating the accuracy of captured snapshots by peer-to-peer crawlers', 'The increasing popularity of Peer-to-Peer (P2P) networks has led to growing interest in characterizing their topology and dynamics [1,2,3,4], essential for proper design and effective evaluation. A common technique is to capture topology snapshots using a crawler. However, previous studies have not verified the accuracy of their captured snapshots. We present techniques to measure the inaccuracy of topology snapshots, quantify the effects of unreachable peers and crawling speed, and explore the impact of snapshot accuracy on derived characterizations.']]"
Per Martin-Löf,[]
Slim Essid,"[['Weakly Supervised Representation Learning for Audio-Visual Scene Analysis', 'Audio-visual (AV) representation learning is an important task from the perspective of designing machines with the ability to understand complex events. To this end, we propose a novel multimodal framework that instantiates multiple instance learning. Specifically, we develop methods that identify events and localize corresponding AV cues in unconstrained videos. Importantly, this is done using weak labels where only video-level event labels are known without any information about their location in time. We show that the learnt representations are useful for performing several tasks such as event/object classification, audio event detection, audio source separation and visual object localization. An important feature of our method is its capacity to learn from unsynchronized audio-visual events. We also demonstrate our framework&#x0027;s ability to separate out the audio source of interest through a novel use of nonnegative matrix factorization. State-of-the-art classification results, with a F1-score of 65.0, are achieved on DCASE 2017 smart cars challenge data with promising generalization to diverse object types such as musical instruments. Visualizations of localized visual regions and audio segments substantiate our system&#x0027;s efficacy, especially when dealing with noisy situations where modality-specific cues appear asynchronously.'], ['UE-HRI: a new dataset for the study of user engagement in spontaneous human-robot interactions', ' In this paper, we present a new dataset of spontaneous interactions between a robot and humans, of which 54 interactions (between 4 and 15-minute duration each) are freely available for download and use. Participants were recorded while holding spontaneous conversations with the robot Pepper. The conversations started automatically when the robot detected the presence of a participant and kept the recording if he/she accepted the agreement (i.e. to be recorded). Pepper was in a public space where the participants were free to start and end the interaction when they wished. The dataset provides rich streams of data that could be used by research and development groups in a variety of areas. '], ['Feature Learning With Matrix Factorization Applied to Acoustic Scene Classification', 'In this paper, we study the usefulness of various matrix factorization methods for learning features to be used for the specific acoustic scene classification ASC problem. A common way of addressing ASC has been to engineer features capable of capturing the specificities of acoustic environments. Instead, we show that better representations of the scenes can be automatically learned from time-frequency representations using matrix factorization techniques. We mainly focus on extensions including sparse, kernel-based, convolutive and a novel supervised dictionary learning variant of principal component analysis and nonnegative matrix factorization. An experimental evaluation is performed on two of the largest ASC datasets available in order to compare and discuss the usefulness of these methods for the task. We show that the unsupervised learning methods provide better representations of acoustic scenes than the best conventional hand-crafted features on both datasets. Furthermore, the introduction of a novel nonnegative supervised matrix factorization model and deep neural networks trained on spectrograms, allow us to reach further improvements.'], ['Analysis of dance movements using gaussian processes: extended abstract', 'This work addresses the Huawei/3DLife Grand Challenge, presenting a novel method for the analysis of dance movements. The approach focuses on the decomposition of the dance movements into elementary motions. Placing this problem into a probabilistic framework, we propose to exploit Gaussian processes to accurately model the different components of the decomposition. The preliminary results, presented in this paper, are very promising. In particular, two applications are considered, illustrating the relevance of the proposed approach, namely the correction of tracking errors and the smoothing of some movements of the teacher to help toward the dance learning.'], ['An audio-driven virtual dance-teaching assistant', 'This work addresses the Huawei/3Dlife Grand challenge proposing a set of audio tools for a virtual dance-teaching assistant. These tools are meant to help the dance student develop a sense of rhythm to correctly synchronize his/her movements and steps to the musical timing of the choreographies to be executed. They consist of three main components, namely a music (beat) analysis module, a source separation and remastering module and a dance step segmentation module. These components enable to create augmented tutorial videos highlighting the rhythmic information using, for instance, a synthetic dance teacher voice, but also videos highlighting the steps executed by a student to help in the evaluation of his/her performance.'], ['Enhanced visualisation of dance performance from automatically synchronised multimodal recordings', 'The Huawei/3DLife Grand Challenge Dataset provides multimodal recordings of Salsa dancing, consisting of audiovisual streams along with depth maps and inertial measurements. In this paper, we propose a system for augmented reality-based evaluations of Salsa dancer performances. An essential step for such a system is the automatic temporal synchronisation of the multiple modalities captured from different sensors, for which we propose efficient solutions. Furthermore, we contribute modules for the automatic analysis of dance performances and present an original software application, specifically designed for the evaluation scenario considered, which enables an enhanced dance visualisation experience, through the augmentation of the original media with the results of our automatic analyses.'], ['A conditional random field viewpoint of symbolic audio-to-score matching', 'We present a new approach of symbolic audio-to-score alignment, with the use of Conditional Random Fields (CRFs). Unlike Hidden Markov Models, these graphical models allow the calculation of state conditional probabilities to be made on the basis of several audio frames. The CRF models that we propose exploit this property to take into account the rhythmic information of the musical score. Assuming that the tempo is locally constant, they confront the neighborhood of each frame with several tempo hypotheses. Experiments on a pop-music database show that this use of contextual information leads to a significant improvement of the alignment accuracy. In particular, the proportion of detected onsets inside a 100-ms tolerance window increases by more than 10% when a 1-s neighborhood is considered.'], ['Rushes video summarization using a collaborative approach', 'This paper describes the video summarization system developed by the partners of the K-Space European Network of Excellence for the TRECVID 2008 BBC rushes summarization evaluation. We propose an original method based on individual content segmentation and selection tools in a collaborative system. Our system is organized in several steps. First, we segment the video, secondly we identify relevant and redundant segments, and finally, we select a subset of segments to concatenate and build the final summary with video acceleration incorporated. We analyze the performance of our system through the TRECVID evaluation.']]"
Vincent Perlbarg,[]
Hedi Tabia,"[['The Shape Space of 3D Botanical Tree Models', 'We propose an algorithm for generating novel 3D tree model variations from existing ones via geometric and structural blending. Our approach is to treat botanical trees as elements of a tree-shape space equipped with a proper metric that quantifies geometric and structural deformations. Geodesics, or shortest paths under the metric, between two points in the tree-shape space correspond to optimal deformations that align one tree onto another, including the possibility of expanding, adding, or removing branches and parts. Central to our approach is a mechanism for computing correspondences between trees that have different structures and a different number of branches. The ability to compute geodesics and their lengths enables us to compute continuous blending between botanical trees, which, in turn, facilitates statistical analysis, such as the computation of averages of tree structures. We show a variety of 3D tree models generated with our approach from 3D trees exhibiting complex geometric and structural differences. We also demonstrate the application of the framework in reflection symmetry analysis and symmetrization of botanical trees.'], ['Learning features combination for human action recognition from skeleton sequences', 'A new framework for human action recognition from skeleton sequences.A metric learning method is applied to learn features combination.The proposed approach achieved state-of-the-art results on three datasets.Individual contributions of each stage were demonstrated in the experiments. Human action recognition is a challenging task due to the complexity of human movements and to the variety among the same actions performed by distinct subjects. Recent technologies provide the skeletal representation of human body extracted in real time from depth maps, which is a high discriminant information for efficient action recognition. In this context, we present a new framework for human action recognition from skeleton sequences. We propose extracting sets of spatial and temporal local features from subgroups of joints, which are aggregated by a robust method based on the VLAD algorithm and a pool of clusters. Several feature vectors are then combined by a metric learning method inspired by the LMNN algorithm with the objective to improve the classification accuracy using the nonparametric k-NN classifier. We evaluated our method on three public datasets, including the MSR-Action3D, the UTKinect-Action3D, and the Florence 3D Actions dataset. As a result, the proposed framework performance overcomes the methods in the state of the art on all the experiments.'], ['3D facial expression recognition using kernel methods on Riemannian manifold', 'Automatic human Facial Expressions Recognition (FER) is becoming of increased interest. FER finds its applications in many emerging areas such as affective computing and intelligent human computer interaction. Most of the existing work on FER has been done using 2D data which suffers from inherent problems of illumination changes and pose variations. With the development of 3D image capturing technologies, the acquisition of 3D data is becoming a more feasible task. The 3D data brings a more effective solution in addressing the issues raised by its 2D counterpart. State-of-the-art 3D FER methods are often based on a single descriptor which may fail to handle the large inter-class and intra-class variability of the human facial expressions. In this work, we explore, for the first time, the usage of covariance matrices of descriptors, instead of the descriptors themselves, in 3D FER. Since covariance matrices are elements of the non-linear manifold of Symmetric Positive Definite (SPD) matrices, we particularly look at the application of manifold-based classification to the problem of 3D FER. We evaluate the performance of the proposed framework on the BU-3DFE and the Bosphorus datasets, and demonstrate its superiority compared to the state-of-the-art methods.'], ['Learning shape retrieval from different modalities', 'New shape retrieval framework using queries of different modalities is proposed.Kernel function computed from 3D shape similarity is used to build a common space.CNNs are used to embed three different entities into a common space.A novel 3D shape descriptor based on local CNN features is proposed.We demonstrate the performance of our framework using different benchmarks. We propose in this paper a new framework for 3D shape retrieval using queries of different modalities, which can include 3D models, images and sketches. The main scientific challenge is that different modalities have different representations and thus lie in different spaces. Moreover, the features that can be extracted from 2D images or 2D sketches are often different from those that can be computed from 3D models. Our solution is a new method based on Convolutional Neural Networks (CNN) that embeds all these entities into a common space. We propose a novel 3D shape descriptor based on local CNN features encoded using vectors of locally aggregated descriptors instead of conventional global CNN. Using a kernel function computed from 3D shape similarity, we build a target space in which wild images and sketches can be projected via two different CNNs. With this construction, matching can be performed in the common target space between same entities (sketchsketch, imageimage and 3D shape3D shape) and more importantly across different entities (sketch-image, sketch-3D shape and image-3D shape). We demonstrate the performance of the proposed framework using different benchmarks including large scale SHREC 3D datasets.'], ['Modeling and Exploring Co-variations in the Geometry and Configuration of Man-made 3D Shape Families', 'We introduce co-variation analysis as a tool for modeling the way part geometries and configurations co-vary across a family of man-made 3D shapes. While man-made 3D objects exhibit large geometric and structural variations, the geometry, structure, and configuration of their individual components usually do not vary independently from each other but in a correlated fashion. The size of the body of an airplane, for example, constrains the range of deformations its wings can undergo to ensure that the entire object remains a functionally-valid airplane. These co-variation constraints, which are often non-linear, can be either physical, and thus they can be explicitly enumerated, or implicit to the design and style of the shape family. In this article, we propose a data-driven approach, which takes pre-segmented 3D shapes with known component-wise correspondences and learns how various geometric and structural properties of their components co-vary across the set. We demonstrate, using a variety of 3D shape families, the utility of the proposed co-variation analysis in various applications including 3D shape repositories exploration and shape editing where the propagation of deformations is guided by the co-variation analysis. We also show that the framework can be used for context-guided orientation of objects in 3D scenes.'], ['Using the conflict in Dempster-Shafer evidence theory as a rejection criterion in classifier output combination for 3D human action recognition', 'In this paper, we propose a comprehensive solution to 3D human action recognition including feature extraction, classification, and multiple classifier combination. We effectively present two feature extraction methods, four different types of well-known classifiers, and four multiple classifier combination strategies including a specially designed belief based method. In order to enhance the recognition accuracy, we propose a new rejection criterion based on the conflict from the information sources: the classifier outputs. We test our method on the MSRAction 3D dataset. Discarding examples using the conflict based criterion shows superior results than other combination approaches. Moreover this criterion allows choosing a tradeoff between the performance and rejection rate. A comprehensive solution to 3D human action recognition is proposed.We propose a belief function based classifier combination.Conflict management approach is used to improve the accuracy.We outperform other state-of-the-art methods.'], ['3D sketch-based 3D shape retrieval', 'Sketch-based 3D shape retrieval has unique representation availability of the queries and vast applications. Therefore, it has received more and more attentions in the research community of content-based 3D object retrieval. However, sketch-based 3D shape retrieval is a challenging research topic due to the semantic gap existing between the inaccurate representation of sketches and accurate representation of 3D models. In order to enrich and advance the study of sketch-based 3D shape retrieval, we initialize the research on 3D sketch-based 3D model retrieval and collect a 3D sketch dataset based on a developed 3D sketching interface which facilitates us to draw 3D sketches in the air while standing in front of a Microsoft Kinect. The objective of this track is to evaluate the performance of different 3D sketch-based 3D model retrieval algorithms using the hand-drawn 3D sketch query dataset and a generic 3D model target dataset. The benchmark contains 300 sketches that are evenly divided into 30 classes, as well as 1258 3D models that are classified into 90 classes. In this track, nine runs have been submitted by five groups and their retrieval performance has been evaluated using seven commonly used retrieval performance metrics. We wish this benchmark, the comparative evaluation results and the corresponding evaluation code will further promote sketch-based 3D shape retrieval and its applications.'], ['Covariance-Based Descriptors for Efficient 3D Shape Matching, Retrieval, and Classification', 'State-of-the-art 3D shape classification and retrieval algorithms, hereinafter referred to as shape analysis, are often based on comparing signatures or descriptors that capture the main geometric and topological properties of 3D objects. None of the existing descriptors, however, achieve best performance on all shape classes. In this article, we explore, for the first time, the usage of covariance matrices of descriptors, instead of the descriptors themselves, in 3D shape analysis. Unlike histogram -based techniques, covariance-based 3D shape analysis enables the fusion and encoding of different types of features and modalities into a compact representation. Covariance matrices, however, are elements of the non-linear manifold of symmetric positive definite (SPD) matrices and thus \\BBL2 metrics are not suitable for their comparison and clustering. In this article, we study geodesic distances on the Riemannian manifold of SPD matrices and use them as metrics for 3D shape matching and recognition. We then: (1) introduce the concepts of bag of covariance (BoC) matrices and spatially-sensitive BoC as a generalization to the Riemannian manifold of SPD matrices of the traditional bag of features framework, and (2) generalize the standard kernel methods for supervised classification of 3D shapes to the space of covariance matrices. We evaluate the performance of the proposed BoC matrices framework and covariance -based kernel methods and demonstrate their superiority compared to their descriptor-based counterparts in various 3D shape matching, retrieval, and classification setups.'], ['3D Shape Classification Using Information Fusion', 'The intent of 3D-model classification is to find categories of similar objects according to their shapes. This task is a challenging and important problem in 3D-mining and shape processing. In this paper, we present a novel method to categorize 3D-objects based on view-based descriptors. The proposed method goes into two stages. The first stage corresponds to the training in which 3D-objects in the same category are processed and a set of representative 2D views is selected, The second stage corresponds to the labelling in which unknown objects are classified using a belief based classifier. The experimental results obtained on the Shrec07 datasets show that the system efficiently performs in categorizing 3D-models.'], ['Belief-Function-Based Framework for Deformable 3D-Shape Retrieval', 'The need for efficient tools to index and retrieve 3D content becomes even more acute. This paper presents a fully automatic 3D-object retrieval method. It consists of two main steps namely shape signature extraction to describe the shape of objects, and similarity computing to compute similarity between objects. In the first step (signature extraction), we use a shape descriptor called geodesic cords. This descriptor can be seen as a probability distribution sampled from a shape function. In the second step (similarity computing), a global distance, based on belief function theory, is computed between each pair wise of descriptors corresponding respectively to an object query and an object from a given database. Experiments on commonly-used benchmarks demonstrate that our method obtains competitive performance compared to 3D-object retrieval methods from the state-of-the-art.'], ['Covariance Descriptors for 3D Shape Matching and Retrieval', 'Several descriptors have been proposed in the past for 3D shape analysis, yet none of them achieves best performance on all shape classes. In this paper we propose a novel method for 3D shape analysis using the covariance matrices of the descriptors rather than the descriptors themselves. Covariance matrices enable efficient fusion of different types of features and modalities. They capture, using the same representation, not only the geometric and the spatial properties of a shape region but also the correlation of these properties within the region. Covariance matrices, however, lie on the manifold of Symmetric Positive Definite (SPD) tensors, a special type of Riemannian manifolds, which makes comparison and clustering of such matrices challenging. In this paper we study covariance matrices in their native space and make use of geodesic distances on the manifold as a dissimilarity measure. We demonstrate the performance of this metric on 3D face matching and recognition tasks. We then generalize the Bag of Features paradigm, originally designed in Euclidean spaces, to the Riemannian manifold of SPD matrices. We propose a new clustering procedure that takes into account the geometry of the Riemannian manifold. We evaluate the performance of the proposed Bag of Covariance Matrices framework on 3D shape matching and retrieval applications and demonstrate its superiority compared to descriptor-based techniques.'], ['Compact vectors of locally aggregated tensors for 3D shape retrieval', 'During the last decade, a significant attention has been paid, by the computer vision and the computer graphics communities, to three dimensional (3D) object retrieval. Shape retrieval methods can be divided into three main steps: the shape descriptors extraction, the shape signatures and their associated similarity measures, and the machine learning relevance functions. While the first and the last points have vastly been addressed in recent years, in this paper, we focus on the second point; presenting a new 3D object retrieval method using a new coding/pooling technique and powerful 3D shape descriptors extracted from 2D views. For a given 3D shape, the approach extracts a very large and dense set of local descriptors. From these descriptors, we build a new shape signature by aggregating tensor products of visual descriptors. The similarity between 3D models can then be efficiently computed with a simple dot product. We further improve the compactness and discrimination power of the descriptor using local Principal Component Analysis on each cluster of descriptors. Experiments on the SHREC 2012 and the McGill benchmarks show that our approach outperforms the state-of-the-art techniques, including other BoF methods, both in compactness of the representation and in the retrieval performance.'], ['A parts-based approach for automatic 3D shape categorization using belief functions', 'Grouping 3D objects into (semantically) meaningful categories is a challenging and important problem in 3D mining and shape processing. Here, we present a novel approach to categorize 3D objects. The method described in this article, is a belief-function-based approach and consists of two stages: the training stage, where 3D objects in the same category are processed and a set of representative parts is constructed, and the labeling stage, where unknown objects are categorized. The experimental results obtained on the Tosca-Sumner and the Shrec07 datasets show that the system efficiently performs in categorizing 3D models.'], ['Non-rigid 3D shape classification using bag-of-feature techniques', 'In this paper, we present a new method for 3D-shape categorization using Bag-of-Feature techniques (BoF). This method is based on vector quantization of invariant descriptors of 3D-object patches. We analyze the performance of two wellknown classifiers: the Naïve Bayes and the SVM. The results show the effectiveness of our approach and prove that the method is robust to non-rigid and deformable shapes, in which the class of transformations may be very wide due to the capability of such shapes to bend and assume different forms.'], [""SHREC'11 track: shape retrieval on non-rigid 3D watertight meshes"", 'Non-rigid 3D shape retrieval has become an important research topic in content-based 3D object retrieval. The aim of this track is to measure and compare the performance of non-rigid 3D shape retrieval methods implemented by different participants around the world. The track is based on a new non-rigid 3D shape benchmark, which contains 600 watertight triangle meshes that are equally classified into 30 categories. In this track, 25 runs have been submitted by 9 groups and their retrieval accuracies were evaluated using 6 commonly-utilized measures.'], ['A New 3D-Matching Method of Nonrigid and Partially Similar Models Using Curve Analysis', 'The 3D-shape matching problem plays a crucial role in many applications, such as indexing or modeling, by example. Here, we present a novel approach to matching 3D objects in the presence of nonrigid transformation and partially similar models. In this paper, we use the representation of surfaces by 3D curves extracted around feature points. Indeed, surfaces are represented with a collection of closed curves, and tools from shape analysis of curves are applied to analyze and to compare curves. The belief functions are used to define a global distance between 3D objects. The experimental results obtained on the TOSCA and the SHREC07 data sets show that the system performs efficiently in retrieving similar 3D models.'], ['Local visual patch for 3d shape retrieval', 'We present a novel method for 3D-object retrieval using Bag of Feature (BoF) approaches [8]. The method starts by selecting and then describing a set of points from the 3D-object. The proposed descriptor is an indexed collection of closed curves in R3 on the 3D-surface. Such descriptor has the advantage of being invariant to different transformations that a shape can undergo. Based on vector quantization, we cluster those descriptors to form a shape vocabulary. Then, each point selected in the object is associated to a cluster (word) in that vocabulary. Finally, a BoF histogram counting the occurrences of every word is computed. In order to assess our method, we used shapes from the TOSCA and Sumner datasets. The results clearly demonstrate that the method is robust to many kind of transformations and produces higher precision compared with some state-of-the-art methods.'], ['3D-Shape Retrieval Using Curves and HMM', 'In this paper, we propose a new approach for 3D-shape matching. This approach encloses an off-line step and an on-line step. In the off-line one, an alphabet, of which any shape can be composed, is constructed. First, 3D-objects are subdivided into a set of 3D-parts. The subdivision consists to extract from each object a set of feature points with associated curves. Then the whole set of 3D-parts is clustered into different classes from a semantic point of view. After that, each class is modeled by a Hidden Markov Model (HMM). The HMM, which represents a character in the alphabet, is trained using the set of curves corresponding to the class parts. Hence, any 3D-object can be represented by a set of characters. The on-line step consists to compare the set of characters representing the 3D-object query and that of each object in the given dataset. The experimental results obtained on the TOSCA dataset show that the system efficiently performs in retrieving similar 3D-models.']]"
Koji Nonobe,[]
Sonja Zehetmayer,[]
Alexandrina Orzan,"[['Diffusion curves: a vector representation for smooth-shaded images', 'We describe a new vector-based primitive for creating smooth-shaded images, called the diffusion curve. A diffusion curve partitions the space through which it is drawn, defining different colors on either side. These colors may vary smoothly along the curve. In addition, the sharpness of the color transition from one side of the curve to the other can be controlled. Given a set of diffusion curves, the final image is constructed by solving a Poisson equation whose constraints are specified by the set of gradients across all diffusion curves. Like all vector-based primitives, diffusion curves conveniently support a variety of operations, including geometry-based editing, keyframe animation, and ready stylization. Moreover, their representation is compact and inherently resolution independent. We describe a GPU-based implementation for rendering images defined by a set of diffusion curves in real time. We then demonstrate an interactive drawing system for allowing artists to create artworks using diffusion curves, either by drawing the curves in a freehand style, or by tracing existing imagery. Furthermore, we describe a completely automatic conversion process for taking an image and turning it into a set of diffusion curves that closely approximate the original image content.'], ['Diffusion curves: a vector representation for smooth-shaded images', 'We describe a new vector-based primitive for creating smooth-shaded images, called the diffusion curve. A diffusion curve partitions the space through which it is drawn, defining different colors on either side. These colors may vary smoothly along the curve. In addition, the sharpness of the color transition from one side of the curve to the other can be controlled. Given a set of diffusion curves, the final image is constructed by solving a Poisson equation whose constraints are specified by the set of gradients across all diffusion curves. Like all vector-based primitives, diffusion curves conveniently support a variety of operations, including geometry-based editing, keyframe animation, and ready stylization. Moreover, their representation is compact and inherently resolution-independent. We describe a GPU-based implementation for rendering images defined by a set of diffusion curves in realtime. We then demonstrate an interactive drawing system for allowing artists to create artworks using diffusion curves, either by drawing the curves in a freehand style, or by tracing existing imagery. The system is simple and intuitive: we show results created by artists after just a few minutes of instruction. Furthermore, we describe a completely automatic conversion process for taking an image and turning it into a set of diffusion curves that closely approximate the original image content.'], ['Diffusion curves: a vector representation for smooth-shaded images', 'We describe a new vector-based primitive for creating smooth-shaded images, called the diffusion curve. A diffusion curve partitions the space through which it is drawn, defining different colors on either side. These colors may vary smoothly along the curve. In addition, the sharpness of the color transition from one side of the curve to the other can be controlled. Given a set of diffusion curves, the final image is constructed by solving a Poisson equation whose constraints are specified by the set of gradients across all diffusion curves. Like all vector-based primitives, diffusion curves conveniently support a variety of operations, including geometry-based editing, keyframe animation, and ready stylization. Moreover, their representation is compact and inherently resolution-independent. We describe a GPU-based implementation for rendering images defined by a set of diffusion curves in realtime. We then demonstrate an interactive drawing system for allowing artists to create artworks using diffusion curves, either by drawing the curves in a freehand style, or by tracing existing imagery. The system is simple and intuitive: we show results created by artists after just a few minutes of instruction. Furthermore, we describe a completely automatic conversion process for taking an image and turning it into a set of diffusion curves that closely approximate the original image content.'], ['Multi-scale shape manipulations in photographs', 'No abstract available.'], ['Structure-preserving manipulation of photographs', 'Visual content is often better communicated by simplified or exaggerated images than by the ""real world like"" images. In this paper, we offer a tool for creating such enhanced representations of photographs in a way consistent with the original image content. To do so, we develop a method to identify the relevant image structures and their importance. Our approach (a) uses edges as the basic structural unit in the image, (b) proposes tools to manipulate this structure in a flexible way, and (c) employs gradient domain image processing techniques to reconstruct the final image from a ""cropped"" gradient information. This edge-based approach to non-photorealistic image processing is made feasible by two new techniques we introduce: an addition to the Gaussian scale space theory to compute a perceptually meaningful hierarchy of structures, and a contrast estimation method necessary for faithful gradient-based reconstructions. We finally present various applications that manipulate image structure in different ways.']]"
Christian Goerick,"[['Enriching a spatial road representation with lanes and driving directions', 'The detection of lane layout in the surroundings of the ego-vehicle is a key issue for modern ADAS and autonomous driving. Most modern systems rely on annotated spatial maps to provide lane information. However, these maps are not available everywhere, and thus have to be often supported by direct detection systems (e.g. cameras, lasers). Such systems detect lane boundaries by directly observing lane markings or sensing curbstones and pavements. However, well defined boundaries are not always there, or they can be difficult to detect, especially in rural or inner city roads. Furthermore, other traffic participants in the surroundings can significantly limit the field of view of the ego-vehicle. In order to help lane detection in this kind of scenario, indirect cues, such as the behavior of the other vehicles, can be a useful resource. In this paper we propose a grid-based approach, that works on a road terrain representation and assigns a lane and a driving direction to each patch of road. Our approach segments the road terrain into multiple lanes based on geometrical considerations and prior knowledge, and then assigns a driving direction to each lane, taking into account the observed and predicted motion of the other vehicles in the scene. The road terrain is provided by a system we proposed in a previous work, which combines direct and indirect cues to build a probabilistic road representation grid. This new approach constitutes an additional layer to our representation, providing more awareness about the road regulations to motion control systems and allowing them to plan safer trajectories.'], ['Inferring a spatial road representation from the behavior of real world traffic participants', 'The detection of road area in the surroundings of the ego-vehicle is a key issue for modern ADAS. Camera-based direct detection systems are able to reliably accomplish this task only within a limited spatial range or in simple environments, due to hardware limitations and unfavorable situations, like shadows or occlusions. In complex environments, like inner city, traffic is a real issue, since the mere presence of other cars can significantly restrict the field of view of the ego-vehicle. In order to extend the spatial range of road detection, indirect detection systems are a viable resource. They can complement state-of-the-art direct detection systems and help motion control systems to plan smooth and stable trajectories.'], ['Learning Information Acquisition for Multitasking Scenarios in Dynamic Environments', 'Real world environments are so dynamic and unpredictable that a goal-oriented autonomous system performing a set of tasks repeatedly never experiences the same situation even though the task routines are the same. Hence, manually designed solutions to execute such tasks are likely to fail due to such variations. Developmental approaches seek to solve this problem by implementing local learning mechanisms to the systems that can unfold capabilities to achieve a set of tasks through interactions with the environment. However, gathering all the information available in the environment for local learning mechanisms to process is hardly possible due to limited resources of the system. Thus, an information acquisition mechanism is necessary to find task-relevant information sources and applying a strategy to update the knowledge of the system about these sources efficiently in time. A modular systems approach may provide a useful structured and formalized basis for that. In such systems different modules may request access to the constrained system resources to acquire information they are tuned for. We propose a reward-based learning framework that achieves an efficient strategy for distributing the constrained system resources among modules to keep relevant environmental information up to date for higher level task learning and executing mechanisms in the system. We apply the proposed framework to a visual attention problem in a system using the iCub humanoid in simulation.'], ['A hierarchical framework for spectro-temporal feature extraction', 'In this paper we present a hierarchical framework for the extraction of spectro-temporal acoustic features. The design of the features targets higher robustness in dynamic environments. Motivated by the large gap between human and machine performance in such conditions we take inspirations from the organization of the mammalian auditory cortex in the design of our features. This includes the joint processing of spectral and temporal information, the organization in hierarchical layers, competition between coequal features, the use of high-dimensional sparse feature spaces, and the learning of the underlying receptive fields in a data-driven manner. Due to these properties we termed the features as hierarchical spectro-temporal (HIST) features. For the learning of the features at the first layer we use Independent Component Analysis (ICA). At the second layer of our feature hierarchy we apply Non-Negative Sparse Coding (NNSC) to obtain features spanning a larger frequency and time region. We investigate the contribution of the different subparts of this feature extraction process to the overall performance. This includes an analysis of the benefits of the hierarchical processing, the comparison of different feature extraction methods on the first layer, the evaluation of the feature competition, and the investigation of the influence of different receptive field sizes on the second layer. Additionally, we compare our features to MFCC and RASTA-PLP features in a continuous digit recognition task in noise. On a wideband dataset we constructed ourselves based on the Aurora-2 task, as well as on the actual Aurora-2 database. We show that a combination of the proposed HIST features and RASTA-PLP features yields significant improvements and that the proposed features carry complementary information to RASTA-PLP and MFCC features.'], ['Towards an Understanding of Hierarchical Architectures', 'Cognitive systems research aims to understand how cognitive abilities can be created in artificial systems. One key issue is the architecture of the system. It organizes the interplay between the different system elements and thus, determines the principle limits for the performance of the system. In this contribution, we focus on important properties of hierarchical cognitive systems. Therefore, we first present a framework for modeling hierarchical systems. Based on this framework, we formulate and discuss some crucial issues that should be treated explicitly in the design of a system. On this basis, we analyze and compare several well-established cognitive architectures with respect to their internal structure.'], ['A language for formal design of embedded intelligence research systems', 'The construction of complex artifacts of artificial intelligence requires large-scale system integration and collaboration. System architectures are a central issue to enable this process. To develop these, hypotheses must be formulated, validated and evolved. We therefore present Systematica 2d, a formalism suitable for both flexible description of hierarchical architecture concepts as well as functional design of the resulting system integration process. We motivate the approach and relate it to other formal descriptions by means of a new formalization measure. It consists of a set of criteria to evaluate how well a formalism supports the expression, construction and reuse of intelligent systems. Systematica 2d is compared with existing formalization languages under this measure and shown to have at least their level of expression. In addition, the system properties of incremental composition, partial testability and global deadlock-free operation are formally defined and proven in the formalism.'], ['A biologically-inspired vision architecture for resource-constrained intelligent vehicles', 'The use of computer vision for assisting the driver dates back to first research projects in 1980s, but only recently the progress in vision research and the increase in computational power have resulted in actual products. Although impressive from the robustness point of view, these systems are optimized for specific problems and at best perform reactive tasks like, e.g., lane keeping assistance. However, for a better understanding of generic traffic situations and for assisting the driver in the full range of his actions, integrated and more flexible approaches are needed. In this contribution we propose a vision system that in important aspects is inspired by the human visual system for organizing the different visual routines that need to be carried out. The presented system searches for biological motivation in case classical engineering-based approaches cannot do better or fail. Using a tunable visual attention system and state-of-the-art perception algorithms, the system is capable of analyzing the scenery for task-relevant information in order to provide the driver with assistance in dangerous situations. Our main research focus is on the design of general mechanisms (i.e., not domain or task-specific) that lead to a certain observable behavior without being explicitly designed for this behavior. Using this principle, we aim at developing easily extensible driver assistance systems. The system components are evaluated on a complex inner-city scene and on further real-world data. We demonstrate the performance of the integrated vision system in a construction site setup. A traffic jam within the construction site results in a dangerous situation that the system has to identify in order to warn the driver. Different from other systems the detection of the dangerous situation is based on the vision channel alone. Radar is only used to assign distance data to visually detected objects. The contribution represents an important intermediate stage for future, more cognitive driver assistance systems.'], ['Combining Auditory Preprocessing and Bayesian Estimation for Robust Formant Tracking', 'We present a framework for estimating formant trajectories. Its focus is to achieve high robustness in noisy environments. Our approach combines a preprocessing based on functional principles of the human auditory system and a probabilistic tracking scheme. For enhancing the formant structure in spectrograms we use a Gammatone filterbank, a spectral preemphasis, as well as a spectral filtering using difference-of-Gaussians (DoG) operators. Finally, a contrast enhancement mimicking a competition between filter responses is applied. The probabilistic tracking scheme adopts the mixture modeling technique for estimating the joint distribution of formants. In conjunction with an algorithm for adaptive frequency range segmentation as well as Bayesian smoothing an efficient framework for estimating formant trajectories is derived. Comprehensive evaluations of our method on the VTR-formant database emphasize its high precision and robustness. We obtained superior performance compared to existing approaches for clean as well as echoic noisy speech. Finally, an implementation of the framework within the scope of an online system using instantaneous feature-based resynthesis demonstrates its applicability to real-world scenarios.'], ['A Hierarchical System Integration Approach with Application to Visual Scene Exploration for Driver Assistance', 'A scene exploration which is quick and complete according to current task is the foundation for most higher scene processing. Many specialized approaches exist in the driver assistance domain (e.g. car recognition or lane marking detection), but we aim at an integrated system, combining several such techniques to achieve sufficient performance. In this work we present a novel approach to this integration problem. Algorithms are contained in hierarchically arranged layers with the main principle that the ordering is induced by the requirement that each layer depends only on the layers below. Thus, higher layers can be added to a running system (incremental composition) and shutdown or failure of higher layers leaves the system in an operational state, albeit with reduced functionality (graceful degradation). Assumptions, challenges and benefits when applying this approach to practical systems are discussed. We demonstrate our approach on an integrated system performing visual scene exploration on real-world data from a prototype vehicle. System performance is evaluated on two scene exploration completeness measures and shown to gracefully degrade as several layers are removed and to fully recover as these layers are restarted while the system is running.'], ['Decentralized planning for dynamic motion generation of multi-link robotic systems', 'This paper presents a decentralized planning method for generating dynamic whole body motions of multilink robots including humanoids. First, a robotic system will be modeled as a general multi-body dynamical system. The planning problem of a multi-body system will then be formulated as a constraint resolution problem. The problem will be solved by means of an extended Gauss-Seidel method, which is capable of handling multiple constraint groups with different priorities. The method will be demonstrated in whole-body motion generation tasks of a humanoid, both in numerical simulations and in experiments using a real humanoid robot.'], ['Audio proto objects for improved sound localization', 'In this article we present a new framework for auditory processing that combines feature extraction and grouping processes to form what we call audio proto objects. These proto objects combine an arbitrary number of audio features in a compact representation that allows a more precise sound localization and also better interfacing to behavior-control in robotics. We compare our standard sound localization system with the new approach in several scenarios to demonstrate the potential of the new approach.'], ['A dynamic attention system that reorients to unexpected motion in real-world traffic environments', 'In this paper we propose a system architecture that extends the current state-of-the-art in computational visual attention by incorporating the biological concept of ventral attention. According to recent findings regarding the neurobiological foundations of attention, there exist two separate but interacting attention systems in the human brain: the dorsal attention system and the ventral attention system. As opposed to the well-known computational concepts of bottom-up and top-down saliency, which both correspond to the dorsal attention system, the ventral attention system is sensitive to behavior-relevant stimuli that are unexpected (i.e. not top-down salient), independent of their perceptual saliency (bottom-up saliency). This results in a dynamic interplay between top-down saliency, bottom-up saliency and ventral attention in the proposed system architecture, enabling the system to redirect its focus of attention to important stimuli while being absorbed in a task, even if their perceptual saliency is low. Our technical system instance implementing the proposed architecture integrates several state-of-the-art methods in a coherent system and concentrates on unexpected motion as a first technical account of ventral attention. In our experiments, we demonstrate that the ventral attention enables our system to detect and reorient to important situations in real-world traffic environments that are relevant for the behavior of driving.'], ['Fast detection of arbitrary planar surfaces from unreliable 3D data', 'Man-made real-world environments are dominated by planar surfaces many of which constitute behaviorrelevant entities. Thus, the ability to perceive planar surfaces is vital for any embodied system operating in such environments, be it human or robotic. In this paper, we present an architecture for detection and estimation of planar surfaces in the scene from calibrated stereo images. They are represented in a behavior-oriented way, focusing on geometrical properties that are relevant for enabling basic interaction between a robot and the planar surfaces it perceives. Ego-motion of the robot is compensated for by transforming the representations into a global coordinate system using the kinematics of the robot. Our architecture is able to detect and estimate arbitrary planar surfaces, regardless of their visual appearance, their geometrical properties other than planarity and their being static or arbitrarily moving. The latter is achieved by processing each frame independently of the others. Stable representations are obtained by establishing spatio-temporal coherence between the single-frame representations of subsequent frames. Based on a RANSAC approach to plane fitting, our method is robust to unreliable 3D data such as obtained by local stereo correlation, for example. In our experiments using the Honda humanoid robot ASIMO, we show that our method is able to provide a robot in real-time with representations of planar surfaces in its environment that are sufficiently accurate for basic interaction.'], ['Instant prediction for reactive motions with planning', ""Reactive control and planning are complementary methods in robot motion control. The advantage of planning is the ability to find difficult solutions, optimize trajectories globally and not getting stuck in local minima but at higher computational cost. On the other hand, reactive control can handle dynamic or uncertain environments at low computational cost, but may get stuck in local minima. In this paper, we propose a new approach to integrate both reactive control and planning using a short time prediction. The system is mainly composed of a predictor, a planner and a reactive controller. The system uses the planner to modify the target for the reactive controller. The predictor simulates the robot future states and evaluates the reactive motion to trigger the planner in advance. The latency due to the high computational costs for planning is compensated since the motion is already simulated, therefore, the resulting motion is smoother than without the predictor. When the system's environment becomes more uncertain and dynamic, the system works reactively and iterates faster so that the system adapts to the environment automatically. We tested the scheme in a simulator and realized it on our humanoid robot ASIMO.""], ['Automatic selection of task spaces for imitation learning', 'Previous work [1] shows that the movement representation in task spaces offers many advantages for learning object-related and goal-directed movement tasks through imitation. It allows to reduce the dimensionality o f the data that is learned and simplifies the correspondence problem that results from different kinematic structures of teacher and robot. Further, the task space representation provides a first generalization, for example wrt. differing absolute positions, if bi-manual movements are represented in relation to each other. Although task spaces are widely used, even if they are not mentioned explicitly, they are mostly defined a priori. This work is a step towards an automatic selection of task spaces. Observed movements are mapped into a pool of possibly even conflicting task spaces and we present methods that analyze this task space pool in order to acquire task space descriptors that match the observation best. As statistical measures cannot explain importance for all kinds of movements, the presented selection scheme incorporates additional criteria such as an attention-based measure. Further, we introduce methods that make a significant step from purely statistically-driven task space selection towards model-based movement analysis using a simulation of a complex human model. Effort and discomfort of the human teacher is being analyzed and used as a hint for important task elements. All methods are validated with realworld data, gathered using color tracking with a stereo vision system and a VICON motion capturing system.'], ['Robust constraint-consistent learning', 'Many everyday human skills can be framed in terms of performing some task subject to constraints imposed by the environment. Constraints are usually unobservable and frequently change between contexts. In this paper, we present a novel approach for learning (unconstrained) control policies from movement data, where observations are recorded under different constraint settings. Our approach seamlessly integrates unconstrained and constrained observations by performing hybrid optimisation of two risk functionals. The first is a novel risk functional that makes a meaningful comparison between the estimated policy and constrained observations. The second is the standard risk, used to reduce the expected error under impoverished sets of constraints. We demonstrate our approach on systems of varying complexity, and illustrate its utility for transfer learning of a car washing task from human motion capture data.'], ['Self-management for neural dynamics in brain-like information processing', 'Neural dynamics coupled by adaptive synaptic information transmission provide a very powerful tool for biologically inspired visual processing systems[4]. Currently, progress is limited by the computing time needed to evaluate the underlying equations and by the high number of parameters necessary to tune to achieve the desired system performance. In this contribution we apply Autonomic Computing techniques to overcome these limitations. We approach the computing time problem with an error model of the differential equations allowing for self-optimization of the evaluation step size and the parameter problem with a self-configuration heuristics to keep neural activation in working range. We show the equivalence of system behavior compared to the case without self-management, the performance gain achieved by the self-optimization and the stability achieved by the self-configuration.'], ['Learning from a tutor: Embodied speech acquisition and imitation learning', ""This work presents a new developmentally inspired data-driven framework to bootstrap speech perception and imitation abilities in interaction with a tutor. The proposed system architecture extends our work presented in [1], that implements a cascade of interconnected layers to acquire the structure of speech in terms of phones, syllables and words. Here, we show how to couple such a perceptual model with a speech imitation system that is based on an acoustic synthesizer bound to produce speech sounds with a child's voice.""], ['Learning and use of sensorimotor schemata maps', 'In this paper we present a framework for the learning and use of sensorimotor schemata. Therefore, we introduce the concept of a schema as a compact representation of an attractor dynamic and discuss how schemata, if embedded into the proposed architecture, can be used to produce, simulate, or recognize goal-directed behaviors. We further present a first implementation of the framework which incorporates well-founded biological principles. Firstly, we apply population coding for the representation of schemata in a neural map and, secondly, we use basis functions as flexible intermediate representations for sensorimotor transformations. Simulation results show that during an initial motor babbling phase the system is able to autonomously develop schemata which correspond to generic behaviors. Moreover, the learned sensorimotor schemata map is topologically ordered insofar as neighboring schemata represent similar behaviors. In accordance with biological findings on the motor system of vertebrates the schemata form a set of behavior primitives which can be flexibly combined to yield more complex behaviors.'], ['Task-level imitation learning using variance-based movement optimization', ""Recent advances in the field of humanoid robotics increase the complexity of the tasks that such robots can perform. This makes it increasingly difficult and inconvenient to program these tasks manually. Furthermore, humanoid robots, in contrast to industrial robots, should in the distant future behave within a social environment. Therefore, it must be possible to extend the robot's abilities in an easy and natural way. To address these requirements, this work investigates the topic of imitation learning of motor skills. The focus lies on providing a humanoid robot with the ability to learn new bimanual tasks through the observation of object trajectories. For this, an imitation learning framework is presented, which allows the robot to learn the important elements of an observed movement task by application of probabilistic encoding with Gaussian Mixture Models. The learned information is used to initialize an attractor-based movement generation algorithm that optimizes the reproduced movement towards the fulfillment of additional criteria, such as collision avoidance. Experiments performed with the humanoid robot ASIMO show that the proposed system is suitable for transferring information from a human demonstrator to the robot. These results provide a good starting point for more complex and interactive learning tasks.""], ['A novel method for learning policies from constrained motion', 'Many everyday human skills can be framed in terms of performing some task subject to constraints imposed by the environment. Constraints are usually unobservable and frequently change between contexts. In this paper, we present a novel approach for learning (unconstrained) control policies from movement data, where observations come from movements under different constraints. As a key ingredient, we introduce a small but highly effective modification to the standard risk functional, allowing us to make a meaningful comparison between the estimated policy and constrained observations. We demonstrate our approach on systems of varying complexity, including kinematic data from the ASIMO humanoid robot with 27 degrees of freedom.'], ['Towards Cognitive Robotics', 'In this paper we review our research aiming at creating a cognitive humanoid. We describe our understanding of the core elements of a processing architecture for such kind of an artifact. After these conceptual considerations we present our research results on the form of the series of elements and systems that have been researched and created.'], ['Behaviour generation in humanoids by learning potential-based policies from constrained motion', 'Movement generation that is consistent with observed or demonstrated behaviour is an efficient way to seed movement planning in complex, high-dimensional movement systems like humanoid robots. We present a method for learning potential-based policies from constrained motion data. In contrast to previous approaches to direct policy learning, our method can combine observations from a variety of contexts where different constraints are in force, to learn the underlying unconstrained policy in form of its potential function. This allows us to generalise and predict behaviour where novel constraints apply. We demonstrate our approach on systems of varying complexity, including kinematic data from the ASIMO humanoid robot with 22 degrees of freedom.'], ['Enhancing Topology Preservation during Neural Field Development Via Wiring Length Minimization', ' We recently proposed a recurrent neural network model for the development of dynamic neural fields [1]. The learning regime incorporates homeostatic processes, such that the network is able to self-organize and maintain a stable operation mode even in face of experience-driven changes in synaptic strengths. However, the learned mappings do not necessarily have to be topology preserving. Here we extend our model by incorporating another mechanism which changes the positions of neurons in the output space. This algorithm operates with a purely local objective function of minimizing the wiring length and runs in parallel to the above mentioned learning process. We experimentally show that the incorporation of this additional mechanism leads to a significant decrease in topological defects and further enhances the quality of the learned mappings. Additionally, the proposed algorithm is not limited to our network model; rather it can be applied to any type of self-organizing maps. '], ['Estimating Object Proper Motion Using Optical Flow, Kinematics, and Depth Information', 'For the interaction of a mobile robot with a dynamic environment, the estimation of object motion is desired while the robot is walking and/or turning its head. In this paper, we describe a system which manages this task by combining depth from a stereo camera and computation of the camera movement from robot kinematics in order to stabilize the camera images. Moving objects are detected by applying optical flow to the stabilized images followed by a filtering method, which incorporates both prior knowledge about the accuracy of the measurement and the uncertainties of the measurement process itself. The efficiency of this system is demonstrated in a dynamic real-world scenario with a walking humanoid robot.'], ['Enhancing robustness of a saliency-based attention system for driver assistance', 'Biologically motivated attention systems prefilter the visual environment for scene elements that pop out most or match the current system task best. However, the robustness of biological attention systems is difficult to achieve, given e.g., the high variability of scene content, changes in illumination, and scene dynamics. Most computational attention models do not show real time capability or are tested in a controlled indoor environment only. No approach is so far used in the highly dynamic real world scenario car domain. Dealing with such scenarios requires a strong system adaptation capability with respect to changes in the environment. Here, we focus on five conceptual issues crucial for closing the gap between artificial and natural attention systems operating in the real world. We show the feasibility of our approach on vision data from the car domain. The described attention system is part of a biologically motivated advanced driver assistance system running in real time.'], ['Researching and developing a real-time infrastructure for intelligent systems - Evolution of an integrated approach', 'In this paper, we describe the principles and the methodologies that we have researched for the creation of a software infrastructure for bridging the gap from brain-like systems design to standard software technology. Looking at the brain, we constantly take inspiration and choose the relevant principles that our computer-base model should/could be based on. This ranges from the evolution of the brain (phylogenetically and ontogenetically), the inherent autonomy of the currently identified areas, the intrinsic synchronization through the most basic control mechanisms that regulates interaction, communication, and modulation. With these principles in mind, we started to make a subdivision of our system into instance, functional and computing architecture, modeling each sub-system with processes and tools in order to create a basic infrastructure that supports the research and creation of intelligent systems. The basic elements of our infrastructure are the BBCM (Brain Bytes Component Model) and BBDM (Brain Bytes Data Model), created to enable the modularization and reuse of our systems. Based on those, we have developed DTBOS (Design Tool for Brain Operating System), the design environment for supporting graphical design, RTBOS (Real-Time Brain Operating System), the middleware that supports real-time execution of our modular systems, and CMBOS (Control-Monitor Brain Operating System) to enable the monitoring of running modules. We will show the feasibility of the established environment by shortly describing some of the experimental systems in the area of cognitive robotics that we have created. This will serve to give a more concrete understanding of the dimensions and the type of systems that we have been able to create.'], ['Color object recognition in real-world scenes', 'This work investigates the role of color in object recognition. We approach the problem from a computational perspective by measuring the performance of biologically inspired object recognition methods. As benchmarks, we use image datasets proceeding from a real-world object detection scenario and compare classification performance using color and gray-scale versions of the same datasets. In order to make our results as general as possible, we consider object classes with and without intrinsic color, partitioned into 4 datasets of increasing difficulty and complexity. For the same reason, we use two independent bio-inspired models of object classification which make use of color in different ways. We measure the qualitative dependency of classification performance on classifier type and dataset difficulty (and used color space) and compare to results on gray-scale images. Thus, we are able to draw conclusions about the role and the optimal use of color in classification and find that our results are in good agreement with recent psychophysical results.'], ['Word recognition with a hierarchical neural network', 'In this paper we propose a feedforward neural network for syllable recognition. The core of the recognition system is based on a hierarchical architecture initially developed for visual object recognition. We show that, given the similarities between the primary auditory and visual cortexes, such a system can successfully be used for speech recognition. Syllables are used as basic units for the recognition. Their spectrograms, computed using a Gammatone filterbank, are interpreted as images and subsequently feed into the neural network after a preprocessing step that enhances the formant frequencies and normalizes the length of the syllables. The performance of our system has been analyzed on the recognition of 25 different monosyllabic words. The parameters of the architecture have been optimized using an evolutionary strategy. Compared to the Sphinx-4 speech recognition system, our system achieves better robustness and generalization capabilities in noisy conditions.'], ['A biologically motivated system for unconstrained online learning of visual objects', 'We present a biologically motivated system for object recognition that is capable of online learning of several objects based on interaction with a human teacher. The training is unconstrained in the sense that arbitrary objects can be freely presented in front of a stereo camera system and labeled by speech input. The architecture unites biological principles such as appearance-based representation in topographical feature detection hierarchies and context-driven transfer between different levels of object memory. The learning is fully online and thus avoids an artificial separation of the interaction into training and test phases.'], ['Analyzing Learning Dynamics: How to Average?', 'Pattern-based learning processes are usually analyzed by means of probability density functions of the weights or moments thereof. During the derivation of these equations, some averaging has to be performed. In this paper, we will show that the manner of averaging is crucial for the results of the analysis. We will do this by comparing two types of analysis (Langevin type and discrete-time moments) for one learning system.'], ['From Neural Networks to Neural Strategies', 'No abstract available.'], ['Artificial neural networks in real-time car detection and tracking applications', 'No abstract available.'], ['Über nicht lernbare Probleme oder Ein Modell für die vorzeitige Sättigung bei vorwärtsgekoppelten Neuronalen Netzen', 'No abstract available.']]"
